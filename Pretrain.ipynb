{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Training Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import torch\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import torch.nn.functional as F\n",
    "import torch.nn\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms as T \n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchsummary\n",
    "\n",
    "import copy\n",
    "\n",
    "from timm.data import Mixup\n",
    "from timm.loss import LabelSmoothingCrossEntropy, SoftTargetCrossEntropy\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "config={    \n",
    "    'data_test_dir': r'D:\\Coding\\dataset\\gender_kaggle\\Test',\n",
    "    'data_train_dir': r'D:\\Coding\\dataset\\gender_kaggle\\Train',\n",
    "    'data_single_dir': r'D:\\Coding\\dataset\\flowers',\n",
    "    \n",
    "    'batch_size':64,\n",
    "    'num_classes':2 ,\n",
    "    'epochs': 300\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Torch Transform -> Data Augmentation\n",
    "'''\n",
    "transform = T.Compose([\n",
    "    #T.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5),\n",
    "    # T.RandomApply(torch.nn.ModuleList([T.ColorJitter()]), p=0.25),\n",
    "    # T.RandomAffine(degrees=15, translate=(0.2, 0.2),scale=(0.8, 1.2), shear=15),\n",
    "    # T.RandomHorizontalFlip(),\n",
    "    # T.RandomVerticalFlip(),\n",
    "    # T.RandomRotation(10),\n",
    "    T.RandAugment(num_ops = 9, magnitude = 3),\n",
    "    T.Resize((224,224)),\n",
    "    #T.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)), # imagenet means\n",
    "    T.RandomErasing(p=0.25, value='random')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ImageFolder : 실제로 불러오는 것은 파일별 경로 및 레이블\n",
    "Dataset Shape -> dataset[data_index][0]:label, dataset[data_index][1]:image\n",
    "테스트 데이터셋과 트레이닝 데이터셋을 따로 불러옴\n",
    "'''\n",
    "train_ds = datasets.ImageFolder(root=config['data_train_dir'], transform =transform)\n",
    "test_ds = datasets.ImageFolder(root=config['data_test_dir'], transform =T.Compose([T.Resize((224,224)),T.ToTensor(),T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)) ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# 트레이닝 데이터셋만 있을 경우 ImageFolder에서 train_ds만 불러온 후\\n# Subsampler을 활용하여 검증 데이터셋과 트레이닝 데이터셋, 테스트 데이터셋으로 나눔\\n\\ndef get_sampler(data_len, val):\\n    # data_len 은 트레이닝 데이터셋의 길이\\n    valid_size = 0.1\\n    data_len = len(train_ds)\\n\\n\\n    indices = list(range(data_len)) # 전체 인덱스\\n    np.random.shuffle(indices)\\n\\n    split = int(np.floor(valid_size * data_len))\\n    train_idx, valid_idx,test_idx = indices[:-split*2], indices[-split*2:-split], indices[-split:]\\n\\n    # trainning, validation batch를 얻기 위한 sampler정의\\n    train_sampler = SubsetRandomSampler(train_idx)\\n    valid_sampler = SubsetRandomSampler(valid_idx)\\n    test_sampler = SubsetRandomSampler(test_idx)\\n\\n    train_len = len(train_idx)\\n    val_len = len(valid_idx)\\n    test_len = len(test_idx)\\n\\n    return train_sampler, valid_sampler, test_sampler, train_len, val_len, test_len\\n\\ntrain_ds = datasets.ImageFolder(root=config['data_single_dir'], transform =transform)\\ntrain_idx, valid_idx, test_idx, train_len, val_len, test_len = get_sampler(data_len=len(train_ds), val=0.1)\\n\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# 트레이닝 데이터셋만 있을 경우 ImageFolder에서 train_ds만 불러온 후\n",
    "# Subsampler을 활용하여 검증 데이터셋과 트레이닝 데이터셋, 테스트 데이터셋으로 나눔\n",
    "\n",
    "def get_sampler(data_len, val):\n",
    "    # data_len 은 트레이닝 데이터셋의 길이\n",
    "    valid_size = 0.1\n",
    "    data_len = len(train_ds)\n",
    "\n",
    "\n",
    "    indices = list(range(data_len)) # 전체 인덱스\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    split = int(np.floor(valid_size * data_len))\n",
    "    train_idx, valid_idx,test_idx = indices[:-split*2], indices[-split*2:-split], indices[-split:]\n",
    "\n",
    "    # trainning, validation batch를 얻기 위한 sampler정의\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "    test_sampler = SubsetRandomSampler(test_idx)\n",
    "\n",
    "    train_len = len(train_idx)\n",
    "    val_len = len(valid_idx)\n",
    "    test_len = len(test_idx)\n",
    "\n",
    "    return train_sampler, valid_sampler, test_sampler, train_len, val_len, test_len\n",
    "\n",
    "train_ds = datasets.ImageFolder(root=config['data_single_dir'], transform =transform)\n",
    "train_idx, valid_idx, test_idx, train_len, val_len, test_len = get_sampler(data_len=len(train_ds), val=0.1)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntrain_loader = DataLoader(train_idx, batch_size=config['batch_size'],sampler=None ,shuffle=True, num_workers=0)\\nval_loader = DataLoader(valid_idx, batch_size=config['batch_size'],sampler=None,shuffle=True, num_workers=0)\\n\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "train_loader = DataLoader(train_idx, batch_size=config['batch_size'],sampler=None ,shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(valid_idx, batch_size=config['batch_size'],sampler=None,shuffle=True, num_workers=0)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "DataLoader : 데이터로더는 데이터셋을 배치 단위로 불러올 수 있게 해주는 역할 -> 경로를 기반으로 실제로 데이터를 배치단위로 불러옴\n",
    "\n",
    "sampler -> 샘플 인덱스 지정해주는 역할 (이전의 SubsetRandomSampler의 결과가 인풋)\n",
    "shuffle -> 셔플링 여부 \n",
    "둘중에 하나만 쓰는 것이 좋음\n",
    "num_workers -> 멀티프로세싱을 몇개로 해줄지: 0이면 싱글프로세싱, 주로 CPU 개수 만큼 설정합니다\n",
    "'''\n",
    "train_loader = DataLoader(train_ds, batch_size=config['batch_size'],sampler=None ,shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(test_ds, batch_size=config['batch_size'],sampler=None,shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 허깅페이스 사전학습 모델 로딩\n",
    "model = timm.create_model('vit_small_patch16_224', pretrained=True, num_classes=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 384, 14, 14]         295,296\n",
      "          Identity-2             [-1, 196, 384]               0\n",
      "        PatchEmbed-3             [-1, 196, 384]               0\n",
      "           Dropout-4             [-1, 197, 384]               0\n",
      "          Identity-5             [-1, 197, 384]               0\n",
      "          Identity-6             [-1, 197, 384]               0\n",
      "         LayerNorm-7             [-1, 197, 384]             768\n",
      "            Linear-8            [-1, 197, 1152]         443,520\n",
      "          Identity-9           [-1, 6, 197, 64]               0\n",
      "         Identity-10           [-1, 6, 197, 64]               0\n",
      "          Dropout-11          [-1, 6, 197, 197]               0\n",
      "           Linear-12             [-1, 197, 384]         147,840\n",
      "          Dropout-13             [-1, 197, 384]               0\n",
      "        Attention-14             [-1, 197, 384]               0\n",
      "         Identity-15             [-1, 197, 384]               0\n",
      "         Identity-16             [-1, 197, 384]               0\n",
      "        LayerNorm-17             [-1, 197, 384]             768\n",
      "           Linear-18            [-1, 197, 1536]         591,360\n",
      "             GELU-19            [-1, 197, 1536]               0\n",
      "          Dropout-20            [-1, 197, 1536]               0\n",
      "         Identity-21            [-1, 197, 1536]               0\n",
      "           Linear-22             [-1, 197, 384]         590,208\n",
      "          Dropout-23             [-1, 197, 384]               0\n",
      "              Mlp-24             [-1, 197, 384]               0\n",
      "         Identity-25             [-1, 197, 384]               0\n",
      "         Identity-26             [-1, 197, 384]               0\n",
      "            Block-27             [-1, 197, 384]               0\n",
      "        LayerNorm-28             [-1, 197, 384]             768\n",
      "           Linear-29            [-1, 197, 1152]         443,520\n",
      "         Identity-30           [-1, 6, 197, 64]               0\n",
      "         Identity-31           [-1, 6, 197, 64]               0\n",
      "          Dropout-32          [-1, 6, 197, 197]               0\n",
      "           Linear-33             [-1, 197, 384]         147,840\n",
      "          Dropout-34             [-1, 197, 384]               0\n",
      "        Attention-35             [-1, 197, 384]               0\n",
      "         Identity-36             [-1, 197, 384]               0\n",
      "         Identity-37             [-1, 197, 384]               0\n",
      "        LayerNorm-38             [-1, 197, 384]             768\n",
      "           Linear-39            [-1, 197, 1536]         591,360\n",
      "             GELU-40            [-1, 197, 1536]               0\n",
      "          Dropout-41            [-1, 197, 1536]               0\n",
      "         Identity-42            [-1, 197, 1536]               0\n",
      "           Linear-43             [-1, 197, 384]         590,208\n",
      "          Dropout-44             [-1, 197, 384]               0\n",
      "              Mlp-45             [-1, 197, 384]               0\n",
      "         Identity-46             [-1, 197, 384]               0\n",
      "         Identity-47             [-1, 197, 384]               0\n",
      "            Block-48             [-1, 197, 384]               0\n",
      "        LayerNorm-49             [-1, 197, 384]             768\n",
      "           Linear-50            [-1, 197, 1152]         443,520\n",
      "         Identity-51           [-1, 6, 197, 64]               0\n",
      "         Identity-52           [-1, 6, 197, 64]               0\n",
      "          Dropout-53          [-1, 6, 197, 197]               0\n",
      "           Linear-54             [-1, 197, 384]         147,840\n",
      "          Dropout-55             [-1, 197, 384]               0\n",
      "        Attention-56             [-1, 197, 384]               0\n",
      "         Identity-57             [-1, 197, 384]               0\n",
      "         Identity-58             [-1, 197, 384]               0\n",
      "        LayerNorm-59             [-1, 197, 384]             768\n",
      "           Linear-60            [-1, 197, 1536]         591,360\n",
      "             GELU-61            [-1, 197, 1536]               0\n",
      "          Dropout-62            [-1, 197, 1536]               0\n",
      "         Identity-63            [-1, 197, 1536]               0\n",
      "           Linear-64             [-1, 197, 384]         590,208\n",
      "          Dropout-65             [-1, 197, 384]               0\n",
      "              Mlp-66             [-1, 197, 384]               0\n",
      "         Identity-67             [-1, 197, 384]               0\n",
      "         Identity-68             [-1, 197, 384]               0\n",
      "            Block-69             [-1, 197, 384]               0\n",
      "        LayerNorm-70             [-1, 197, 384]             768\n",
      "           Linear-71            [-1, 197, 1152]         443,520\n",
      "         Identity-72           [-1, 6, 197, 64]               0\n",
      "         Identity-73           [-1, 6, 197, 64]               0\n",
      "          Dropout-74          [-1, 6, 197, 197]               0\n",
      "           Linear-75             [-1, 197, 384]         147,840\n",
      "          Dropout-76             [-1, 197, 384]               0\n",
      "        Attention-77             [-1, 197, 384]               0\n",
      "         Identity-78             [-1, 197, 384]               0\n",
      "         Identity-79             [-1, 197, 384]               0\n",
      "        LayerNorm-80             [-1, 197, 384]             768\n",
      "           Linear-81            [-1, 197, 1536]         591,360\n",
      "             GELU-82            [-1, 197, 1536]               0\n",
      "          Dropout-83            [-1, 197, 1536]               0\n",
      "         Identity-84            [-1, 197, 1536]               0\n",
      "           Linear-85             [-1, 197, 384]         590,208\n",
      "          Dropout-86             [-1, 197, 384]               0\n",
      "              Mlp-87             [-1, 197, 384]               0\n",
      "         Identity-88             [-1, 197, 384]               0\n",
      "         Identity-89             [-1, 197, 384]               0\n",
      "            Block-90             [-1, 197, 384]               0\n",
      "        LayerNorm-91             [-1, 197, 384]             768\n",
      "           Linear-92            [-1, 197, 1152]         443,520\n",
      "         Identity-93           [-1, 6, 197, 64]               0\n",
      "         Identity-94           [-1, 6, 197, 64]               0\n",
      "          Dropout-95          [-1, 6, 197, 197]               0\n",
      "           Linear-96             [-1, 197, 384]         147,840\n",
      "          Dropout-97             [-1, 197, 384]               0\n",
      "        Attention-98             [-1, 197, 384]               0\n",
      "         Identity-99             [-1, 197, 384]               0\n",
      "        Identity-100             [-1, 197, 384]               0\n",
      "       LayerNorm-101             [-1, 197, 384]             768\n",
      "          Linear-102            [-1, 197, 1536]         591,360\n",
      "            GELU-103            [-1, 197, 1536]               0\n",
      "         Dropout-104            [-1, 197, 1536]               0\n",
      "        Identity-105            [-1, 197, 1536]               0\n",
      "          Linear-106             [-1, 197, 384]         590,208\n",
      "         Dropout-107             [-1, 197, 384]               0\n",
      "             Mlp-108             [-1, 197, 384]               0\n",
      "        Identity-109             [-1, 197, 384]               0\n",
      "        Identity-110             [-1, 197, 384]               0\n",
      "           Block-111             [-1, 197, 384]               0\n",
      "       LayerNorm-112             [-1, 197, 384]             768\n",
      "          Linear-113            [-1, 197, 1152]         443,520\n",
      "        Identity-114           [-1, 6, 197, 64]               0\n",
      "        Identity-115           [-1, 6, 197, 64]               0\n",
      "         Dropout-116          [-1, 6, 197, 197]               0\n",
      "          Linear-117             [-1, 197, 384]         147,840\n",
      "         Dropout-118             [-1, 197, 384]               0\n",
      "       Attention-119             [-1, 197, 384]               0\n",
      "        Identity-120             [-1, 197, 384]               0\n",
      "        Identity-121             [-1, 197, 384]               0\n",
      "       LayerNorm-122             [-1, 197, 384]             768\n",
      "          Linear-123            [-1, 197, 1536]         591,360\n",
      "            GELU-124            [-1, 197, 1536]               0\n",
      "         Dropout-125            [-1, 197, 1536]               0\n",
      "        Identity-126            [-1, 197, 1536]               0\n",
      "          Linear-127             [-1, 197, 384]         590,208\n",
      "         Dropout-128             [-1, 197, 384]               0\n",
      "             Mlp-129             [-1, 197, 384]               0\n",
      "        Identity-130             [-1, 197, 384]               0\n",
      "        Identity-131             [-1, 197, 384]               0\n",
      "           Block-132             [-1, 197, 384]               0\n",
      "       LayerNorm-133             [-1, 197, 384]             768\n",
      "          Linear-134            [-1, 197, 1152]         443,520\n",
      "        Identity-135           [-1, 6, 197, 64]               0\n",
      "        Identity-136           [-1, 6, 197, 64]               0\n",
      "         Dropout-137          [-1, 6, 197, 197]               0\n",
      "          Linear-138             [-1, 197, 384]         147,840\n",
      "         Dropout-139             [-1, 197, 384]               0\n",
      "       Attention-140             [-1, 197, 384]               0\n",
      "        Identity-141             [-1, 197, 384]               0\n",
      "        Identity-142             [-1, 197, 384]               0\n",
      "       LayerNorm-143             [-1, 197, 384]             768\n",
      "          Linear-144            [-1, 197, 1536]         591,360\n",
      "            GELU-145            [-1, 197, 1536]               0\n",
      "         Dropout-146            [-1, 197, 1536]               0\n",
      "        Identity-147            [-1, 197, 1536]               0\n",
      "          Linear-148             [-1, 197, 384]         590,208\n",
      "         Dropout-149             [-1, 197, 384]               0\n",
      "             Mlp-150             [-1, 197, 384]               0\n",
      "        Identity-151             [-1, 197, 384]               0\n",
      "        Identity-152             [-1, 197, 384]               0\n",
      "           Block-153             [-1, 197, 384]               0\n",
      "       LayerNorm-154             [-1, 197, 384]             768\n",
      "          Linear-155            [-1, 197, 1152]         443,520\n",
      "        Identity-156           [-1, 6, 197, 64]               0\n",
      "        Identity-157           [-1, 6, 197, 64]               0\n",
      "         Dropout-158          [-1, 6, 197, 197]               0\n",
      "          Linear-159             [-1, 197, 384]         147,840\n",
      "         Dropout-160             [-1, 197, 384]               0\n",
      "       Attention-161             [-1, 197, 384]               0\n",
      "        Identity-162             [-1, 197, 384]               0\n",
      "        Identity-163             [-1, 197, 384]               0\n",
      "       LayerNorm-164             [-1, 197, 384]             768\n",
      "          Linear-165            [-1, 197, 1536]         591,360\n",
      "            GELU-166            [-1, 197, 1536]               0\n",
      "         Dropout-167            [-1, 197, 1536]               0\n",
      "        Identity-168            [-1, 197, 1536]               0\n",
      "          Linear-169             [-1, 197, 384]         590,208\n",
      "         Dropout-170             [-1, 197, 384]               0\n",
      "             Mlp-171             [-1, 197, 384]               0\n",
      "        Identity-172             [-1, 197, 384]               0\n",
      "        Identity-173             [-1, 197, 384]               0\n",
      "           Block-174             [-1, 197, 384]               0\n",
      "       LayerNorm-175             [-1, 197, 384]             768\n",
      "          Linear-176            [-1, 197, 1152]         443,520\n",
      "        Identity-177           [-1, 6, 197, 64]               0\n",
      "        Identity-178           [-1, 6, 197, 64]               0\n",
      "         Dropout-179          [-1, 6, 197, 197]               0\n",
      "          Linear-180             [-1, 197, 384]         147,840\n",
      "         Dropout-181             [-1, 197, 384]               0\n",
      "       Attention-182             [-1, 197, 384]               0\n",
      "        Identity-183             [-1, 197, 384]               0\n",
      "        Identity-184             [-1, 197, 384]               0\n",
      "       LayerNorm-185             [-1, 197, 384]             768\n",
      "          Linear-186            [-1, 197, 1536]         591,360\n",
      "            GELU-187            [-1, 197, 1536]               0\n",
      "         Dropout-188            [-1, 197, 1536]               0\n",
      "        Identity-189            [-1, 197, 1536]               0\n",
      "          Linear-190             [-1, 197, 384]         590,208\n",
      "         Dropout-191             [-1, 197, 384]               0\n",
      "             Mlp-192             [-1, 197, 384]               0\n",
      "        Identity-193             [-1, 197, 384]               0\n",
      "        Identity-194             [-1, 197, 384]               0\n",
      "           Block-195             [-1, 197, 384]               0\n",
      "       LayerNorm-196             [-1, 197, 384]             768\n",
      "          Linear-197            [-1, 197, 1152]         443,520\n",
      "        Identity-198           [-1, 6, 197, 64]               0\n",
      "        Identity-199           [-1, 6, 197, 64]               0\n",
      "         Dropout-200          [-1, 6, 197, 197]               0\n",
      "          Linear-201             [-1, 197, 384]         147,840\n",
      "         Dropout-202             [-1, 197, 384]               0\n",
      "       Attention-203             [-1, 197, 384]               0\n",
      "        Identity-204             [-1, 197, 384]               0\n",
      "        Identity-205             [-1, 197, 384]               0\n",
      "       LayerNorm-206             [-1, 197, 384]             768\n",
      "          Linear-207            [-1, 197, 1536]         591,360\n",
      "            GELU-208            [-1, 197, 1536]               0\n",
      "         Dropout-209            [-1, 197, 1536]               0\n",
      "        Identity-210            [-1, 197, 1536]               0\n",
      "          Linear-211             [-1, 197, 384]         590,208\n",
      "         Dropout-212             [-1, 197, 384]               0\n",
      "             Mlp-213             [-1, 197, 384]               0\n",
      "        Identity-214             [-1, 197, 384]               0\n",
      "        Identity-215             [-1, 197, 384]               0\n",
      "           Block-216             [-1, 197, 384]               0\n",
      "       LayerNorm-217             [-1, 197, 384]             768\n",
      "          Linear-218            [-1, 197, 1152]         443,520\n",
      "        Identity-219           [-1, 6, 197, 64]               0\n",
      "        Identity-220           [-1, 6, 197, 64]               0\n",
      "         Dropout-221          [-1, 6, 197, 197]               0\n",
      "          Linear-222             [-1, 197, 384]         147,840\n",
      "         Dropout-223             [-1, 197, 384]               0\n",
      "       Attention-224             [-1, 197, 384]               0\n",
      "        Identity-225             [-1, 197, 384]               0\n",
      "        Identity-226             [-1, 197, 384]               0\n",
      "       LayerNorm-227             [-1, 197, 384]             768\n",
      "          Linear-228            [-1, 197, 1536]         591,360\n",
      "            GELU-229            [-1, 197, 1536]               0\n",
      "         Dropout-230            [-1, 197, 1536]               0\n",
      "        Identity-231            [-1, 197, 1536]               0\n",
      "          Linear-232             [-1, 197, 384]         590,208\n",
      "         Dropout-233             [-1, 197, 384]               0\n",
      "             Mlp-234             [-1, 197, 384]               0\n",
      "        Identity-235             [-1, 197, 384]               0\n",
      "        Identity-236             [-1, 197, 384]               0\n",
      "           Block-237             [-1, 197, 384]               0\n",
      "       LayerNorm-238             [-1, 197, 384]             768\n",
      "          Linear-239            [-1, 197, 1152]         443,520\n",
      "        Identity-240           [-1, 6, 197, 64]               0\n",
      "        Identity-241           [-1, 6, 197, 64]               0\n",
      "         Dropout-242          [-1, 6, 197, 197]               0\n",
      "          Linear-243             [-1, 197, 384]         147,840\n",
      "         Dropout-244             [-1, 197, 384]               0\n",
      "       Attention-245             [-1, 197, 384]               0\n",
      "        Identity-246             [-1, 197, 384]               0\n",
      "        Identity-247             [-1, 197, 384]               0\n",
      "       LayerNorm-248             [-1, 197, 384]             768\n",
      "          Linear-249            [-1, 197, 1536]         591,360\n",
      "            GELU-250            [-1, 197, 1536]               0\n",
      "         Dropout-251            [-1, 197, 1536]               0\n",
      "        Identity-252            [-1, 197, 1536]               0\n",
      "          Linear-253             [-1, 197, 384]         590,208\n",
      "         Dropout-254             [-1, 197, 384]               0\n",
      "             Mlp-255             [-1, 197, 384]               0\n",
      "        Identity-256             [-1, 197, 384]               0\n",
      "        Identity-257             [-1, 197, 384]               0\n",
      "           Block-258             [-1, 197, 384]               0\n",
      "       LayerNorm-259             [-1, 197, 384]             768\n",
      "        Identity-260                  [-1, 384]               0\n",
      "         Dropout-261                  [-1, 384]               0\n",
      "          Linear-262                  [-1, 100]          38,500\n",
      "================================================================\n",
      "Total params: 21,628,132\n",
      "Trainable params: 21,628,132\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 260.83\n",
      "Params size (MB): 82.50\n",
      "Estimated Total Size (MB): 343.91\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 가장 간단한 형태의 병렬화\n",
    "#model =  torch.nn.DataParallel(model,device_ids=[0,1])\n",
    "\n",
    "'''\n",
    "모델 병렬학습 코드\n",
    "1. torch.nn.DataParallel : \n",
    "\n",
    "장점: 정말 간단\n",
    "\n",
    "단점 : DataParallel은 쓰레드간 GIL 경합, 복제 모델의 반복 당 생성, 산란 입력 및 수집 출력으로 인한 추가적인 오버헤드로 인해 \n",
    "단일 시스템에서도 istributedDataParallel보다 느립니다. 또한 \n",
    "\n",
    "2. torch.nn.DistributedDataParallel : 다중 작업이며 단일 및 다중 기기 학습을 전부 지원\n",
    "\n",
    "장점: 빠르다\n",
    "단점 : 추가적인 함수 작성 필요\n",
    "-> 여러 개의 GPU에서 딥러닝을 실행하려면, 모델을 복사해서 각  GPU에 할당해야한다. \n",
    "-> 그 뒤 batchsize를 batch_size/num_gpu만큼 나눈다. 이것을 scatter 한다고 표현한다. (실제로 scatter 함수가 있다.)\n",
    "\n",
    "각 GPU에서 모델이 입력을 받아 출력하는 것을 forward 한다고 표현하고, 이 출력들을 하나의 GPU로 모은다. 이렇게 여러 tensor들(출력들)을 하나의 device로 모으는 것을 gather라고 한다.\n",
    "'''\n",
    "\n",
    "# model 을 GPU로 올림 model.to('cuda')와 동일\n",
    "model.cuda()\n",
    "\n",
    "# 모델 요약 정보\n",
    "# DataParallel을 사용하면 요약 시 모델사이즈가 GPU 병렬사용 갯수만큼 커짐\n",
    "torchsummary.summary(model,(3,224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cspresnet50', 'cspresnet50d', 'cspresnet50w', 'eca_resnet33ts', 'ecaresnet26t', 'ecaresnet50d', 'ecaresnet50d_pruned', 'ecaresnet50t', 'ecaresnet101d', 'ecaresnet101d_pruned', 'ecaresnet200d', 'ecaresnet269d', 'ecaresnetlight', 'gcresnet33ts', 'gcresnet50t', 'inception_resnet_v2', 'lambda_resnet26rpt_256', 'lambda_resnet26t', 'lambda_resnet50ts', 'legacy_seresnet18', 'legacy_seresnet34', 'legacy_seresnet50', 'legacy_seresnet101', 'legacy_seresnet152', 'nf_ecaresnet26', 'nf_ecaresnet50', 'nf_ecaresnet101', 'nf_resnet26', 'nf_resnet50', 'nf_resnet101', 'nf_seresnet26', 'nf_seresnet50', 'nf_seresnet101', 'resnet10t', 'resnet14t', 'resnet18', 'resnet18d', 'resnet26', 'resnet26d', 'resnet26t', 'resnet32ts', 'resnet33ts', 'resnet34', 'resnet34d', 'resnet50', 'resnet50_gn', 'resnet50c', 'resnet50d', 'resnet50s', 'resnet50t', 'resnet51q', 'resnet61q', 'resnet101', 'resnet101c', 'resnet101d', 'resnet101s', 'resnet152', 'resnet152c', 'resnet152d', 'resnet152s', 'resnet200', 'resnet200d', 'resnetaa34d', 'resnetaa50', 'resnetaa50d', 'resnetaa101d', 'resnetblur18', 'resnetblur50', 'resnetblur50d', 'resnetblur101d', 'resnetrs50', 'resnetrs101', 'resnetrs152', 'resnetrs200', 'resnetrs270', 'resnetrs350', 'resnetrs420', 'resnetv2_50', 'resnetv2_50d', 'resnetv2_50d_evos', 'resnetv2_50d_frn', 'resnetv2_50d_gn', 'resnetv2_50t', 'resnetv2_50x1_bit', 'resnetv2_50x3_bit', 'resnetv2_101', 'resnetv2_101d', 'resnetv2_101x1_bit', 'resnetv2_101x3_bit', 'resnetv2_152', 'resnetv2_152d', 'resnetv2_152x2_bit', 'resnetv2_152x4_bit', 'seresnet18', 'seresnet33ts', 'seresnet34', 'seresnet50', 'seresnet50t', 'seresnet101', 'seresnet152', 'seresnet152d', 'seresnet200d', 'seresnet269d', 'seresnetaa50d', 'skresnet18', 'skresnet34', 'skresnet50', 'skresnet50d', 'tresnet_l', 'tresnet_m', 'tresnet_v2_l', 'tresnet_xl', 'vit_base_resnet26d_224', 'vit_base_resnet50d_224', 'vit_small_resnet26d_224', 'vit_small_resnet50d_s16_224', 'wide_resnet50_2', 'wide_resnet101_2']\n"
     ]
    }
   ],
   "source": [
    "# timm.list_models('*resnet*') 이런 식으로 사전학습 가능 모델 검색\n",
    "resnet_model_list = timm.list_models('*resnet*')\n",
    "print(resnet_model_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bat_resnext26ts.ch_in1k', 'beit_base_patch16_224.in22k_ft_in22k', 'beit_base_patch16_224.in22k_ft_in22k_in1k', 'beit_base_patch16_384.in22k_ft_in22k_in1k', 'beit_large_patch16_224.in22k_ft_in22k', 'beit_large_patch16_224.in22k_ft_in22k_in1k', 'beit_large_patch16_384.in22k_ft_in22k_in1k', 'beit_large_patch16_512.in22k_ft_in22k_in1k', 'beitv2_base_patch16_224.in1k_ft_in1k', 'beitv2_base_patch16_224.in1k_ft_in22k', 'beitv2_base_patch16_224.in1k_ft_in22k_in1k', 'beitv2_large_patch16_224.in1k_ft_in1k', 'beitv2_large_patch16_224.in1k_ft_in22k', 'beitv2_large_patch16_224.in1k_ft_in22k_in1k', 'botnet26t_256.c1_in1k', 'caformer_b36.sail_in1k', 'caformer_b36.sail_in1k_384', 'caformer_b36.sail_in22k', 'caformer_b36.sail_in22k_ft_in1k', 'caformer_b36.sail_in22k_ft_in1k_384', 'caformer_m36.sail_in1k', 'caformer_m36.sail_in1k_384', 'caformer_m36.sail_in22k', 'caformer_m36.sail_in22k_ft_in1k', 'caformer_m36.sail_in22k_ft_in1k_384', 'caformer_s18.sail_in1k', 'caformer_s18.sail_in1k_384', 'caformer_s18.sail_in22k', 'caformer_s18.sail_in22k_ft_in1k', 'caformer_s18.sail_in22k_ft_in1k_384', 'caformer_s36.sail_in1k', 'caformer_s36.sail_in1k_384', 'caformer_s36.sail_in22k', 'caformer_s36.sail_in22k_ft_in1k', 'caformer_s36.sail_in22k_ft_in1k_384', 'cait_m36_384.fb_dist_in1k', 'cait_m48_448.fb_dist_in1k', 'cait_s24_224.fb_dist_in1k', 'cait_s24_384.fb_dist_in1k', 'cait_s36_384.fb_dist_in1k', 'cait_xs24_384.fb_dist_in1k', 'cait_xxs24_224.fb_dist_in1k', 'cait_xxs24_384.fb_dist_in1k', 'cait_xxs36_224.fb_dist_in1k', 'cait_xxs36_384.fb_dist_in1k', 'coat_lite_medium.in1k', 'coat_lite_medium_384.in1k', 'coat_lite_mini.in1k', 'coat_lite_small.in1k', 'coat_lite_tiny.in1k', 'coat_mini.in1k', 'coat_small.in1k', 'coat_tiny.in1k', 'coatnet_0_rw_224.sw_in1k', 'coatnet_1_rw_224.sw_in1k', 'coatnet_2_rw_224.sw_in12k', 'coatnet_2_rw_224.sw_in12k_ft_in1k', 'coatnet_3_rw_224.sw_in12k', 'coatnet_bn_0_rw_224.sw_in1k', 'coatnet_nano_rw_224.sw_in1k', 'coatnet_rmlp_1_rw2_224.sw_in12k', 'coatnet_rmlp_1_rw2_224.sw_in12k_ft_in1k', 'coatnet_rmlp_1_rw_224.sw_in1k', 'coatnet_rmlp_2_rw_224.sw_in1k', 'coatnet_rmlp_2_rw_224.sw_in12k', 'coatnet_rmlp_2_rw_224.sw_in12k_ft_in1k', 'coatnet_rmlp_2_rw_384.sw_in12k_ft_in1k', 'coatnet_rmlp_nano_rw_224.sw_in1k', 'coatnext_nano_rw_224.sw_in1k', 'convformer_b36.sail_in1k', 'convformer_b36.sail_in1k_384', 'convformer_b36.sail_in22k', 'convformer_b36.sail_in22k_ft_in1k', 'convformer_b36.sail_in22k_ft_in1k_384', 'convformer_m36.sail_in1k', 'convformer_m36.sail_in1k_384', 'convformer_m36.sail_in22k', 'convformer_m36.sail_in22k_ft_in1k', 'convformer_m36.sail_in22k_ft_in1k_384', 'convformer_s18.sail_in1k', 'convformer_s18.sail_in1k_384', 'convformer_s18.sail_in22k', 'convformer_s18.sail_in22k_ft_in1k', 'convformer_s18.sail_in22k_ft_in1k_384', 'convformer_s36.sail_in1k', 'convformer_s36.sail_in1k_384', 'convformer_s36.sail_in22k', 'convformer_s36.sail_in22k_ft_in1k', 'convformer_s36.sail_in22k_ft_in1k_384', 'convit_base.fb_in1k', 'convit_small.fb_in1k', 'convit_tiny.fb_in1k', 'convmixer_768_32.in1k', 'convmixer_1024_20_ks9_p14.in1k', 'convmixer_1536_20.in1k', 'convnext_atto.d2_in1k', 'convnext_atto_ols.a2_in1k', 'convnext_base.clip_laion2b', 'convnext_base.clip_laion2b_augreg', 'convnext_base.clip_laion2b_augreg_ft_in1k', 'convnext_base.clip_laion2b_augreg_ft_in12k', 'convnext_base.clip_laion2b_augreg_ft_in12k_in1k', 'convnext_base.clip_laion2b_augreg_ft_in12k_in1k_384', 'convnext_base.clip_laiona', 'convnext_base.clip_laiona_320', 'convnext_base.clip_laiona_augreg_320', 'convnext_base.clip_laiona_augreg_ft_in1k_384', 'convnext_base.fb_in1k', 'convnext_base.fb_in22k', 'convnext_base.fb_in22k_ft_in1k', 'convnext_base.fb_in22k_ft_in1k_384', 'convnext_femto.d1_in1k', 'convnext_femto_ols.d1_in1k', 'convnext_large.fb_in1k', 'convnext_large.fb_in22k', 'convnext_large.fb_in22k_ft_in1k', 'convnext_large.fb_in22k_ft_in1k_384', 'convnext_large_mlp.clip_laion2b_augreg', 'convnext_large_mlp.clip_laion2b_augreg_ft_in1k', 'convnext_large_mlp.clip_laion2b_augreg_ft_in1k_384', 'convnext_large_mlp.clip_laion2b_augreg_ft_in12k_384', 'convnext_large_mlp.clip_laion2b_ft_320', 'convnext_large_mlp.clip_laion2b_ft_soup_320', 'convnext_large_mlp.clip_laion2b_soup_ft_in12k_320', 'convnext_large_mlp.clip_laion2b_soup_ft_in12k_384', 'convnext_large_mlp.clip_laion2b_soup_ft_in12k_in1k_320', 'convnext_large_mlp.clip_laion2b_soup_ft_in12k_in1k_384', 'convnext_nano.d1h_in1k', 'convnext_nano.in12k', 'convnext_nano.in12k_ft_in1k', 'convnext_nano_ols.d1h_in1k', 'convnext_pico.d1_in1k', 'convnext_pico_ols.d1_in1k', 'convnext_small.fb_in1k', 'convnext_small.fb_in22k', 'convnext_small.fb_in22k_ft_in1k', 'convnext_small.fb_in22k_ft_in1k_384', 'convnext_small.in12k', 'convnext_small.in12k_ft_in1k', 'convnext_small.in12k_ft_in1k_384', 'convnext_tiny.fb_in1k', 'convnext_tiny.fb_in22k', 'convnext_tiny.fb_in22k_ft_in1k', 'convnext_tiny.fb_in22k_ft_in1k_384', 'convnext_tiny.in12k', 'convnext_tiny.in12k_ft_in1k', 'convnext_tiny.in12k_ft_in1k_384', 'convnext_tiny_hnf.a2h_in1k', 'convnext_xlarge.fb_in22k', 'convnext_xlarge.fb_in22k_ft_in1k', 'convnext_xlarge.fb_in22k_ft_in1k_384', 'convnext_xxlarge.clip_laion2b_rewind', 'convnext_xxlarge.clip_laion2b_soup', 'convnext_xxlarge.clip_laion2b_soup_ft_in1k', 'convnext_xxlarge.clip_laion2b_soup_ft_in12k', 'convnextv2_atto.fcmae', 'convnextv2_atto.fcmae_ft_in1k', 'convnextv2_base.fcmae', 'convnextv2_base.fcmae_ft_in1k', 'convnextv2_base.fcmae_ft_in22k_in1k', 'convnextv2_base.fcmae_ft_in22k_in1k_384', 'convnextv2_femto.fcmae', 'convnextv2_femto.fcmae_ft_in1k', 'convnextv2_huge.fcmae', 'convnextv2_huge.fcmae_ft_in1k', 'convnextv2_huge.fcmae_ft_in22k_in1k_384', 'convnextv2_huge.fcmae_ft_in22k_in1k_512', 'convnextv2_large.fcmae', 'convnextv2_large.fcmae_ft_in1k', 'convnextv2_large.fcmae_ft_in22k_in1k', 'convnextv2_large.fcmae_ft_in22k_in1k_384', 'convnextv2_nano.fcmae', 'convnextv2_nano.fcmae_ft_in1k', 'convnextv2_nano.fcmae_ft_in22k_in1k', 'convnextv2_nano.fcmae_ft_in22k_in1k_384', 'convnextv2_pico.fcmae', 'convnextv2_pico.fcmae_ft_in1k', 'convnextv2_tiny.fcmae', 'convnextv2_tiny.fcmae_ft_in1k', 'convnextv2_tiny.fcmae_ft_in22k_in1k', 'convnextv2_tiny.fcmae_ft_in22k_in1k_384', 'crossvit_9_240.in1k', 'crossvit_9_dagger_240.in1k', 'crossvit_15_240.in1k', 'crossvit_15_dagger_240.in1k', 'crossvit_15_dagger_408.in1k', 'crossvit_18_240.in1k', 'crossvit_18_dagger_240.in1k', 'crossvit_18_dagger_408.in1k', 'crossvit_base_240.in1k', 'crossvit_small_240.in1k', 'crossvit_tiny_240.in1k', 'cs3darknet_focus_l.c2ns_in1k', 'cs3darknet_focus_m.c2ns_in1k', 'cs3darknet_l.c2ns_in1k', 'cs3darknet_m.c2ns_in1k', 'cs3darknet_x.c2ns_in1k', 'cs3edgenet_x.c2_in1k', 'cs3se_edgenet_x.c2ns_in1k', 'cs3sedarknet_l.c2ns_in1k', 'cs3sedarknet_x.c2ns_in1k', 'cspdarknet53.ra_in1k', 'cspresnet50.ra_in1k', 'cspresnext50.ra_in1k', 'darknet53.c2ns_in1k', 'darknetaa53.c2ns_in1k', 'davit_base.msft_in1k', 'davit_small.msft_in1k', 'davit_tiny.msft_in1k', 'deit3_base_patch16_224.fb_in1k', 'deit3_base_patch16_224.fb_in22k_ft_in1k', 'deit3_base_patch16_384.fb_in1k', 'deit3_base_patch16_384.fb_in22k_ft_in1k', 'deit3_huge_patch14_224.fb_in1k', 'deit3_huge_patch14_224.fb_in22k_ft_in1k', 'deit3_large_patch16_224.fb_in1k', 'deit3_large_patch16_224.fb_in22k_ft_in1k', 'deit3_large_patch16_384.fb_in1k', 'deit3_large_patch16_384.fb_in22k_ft_in1k', 'deit3_medium_patch16_224.fb_in1k', 'deit3_medium_patch16_224.fb_in22k_ft_in1k', 'deit3_small_patch16_224.fb_in1k', 'deit3_small_patch16_224.fb_in22k_ft_in1k', 'deit3_small_patch16_384.fb_in1k', 'deit3_small_patch16_384.fb_in22k_ft_in1k', 'deit_base_distilled_patch16_224.fb_in1k', 'deit_base_distilled_patch16_384.fb_in1k', 'deit_base_patch16_224.fb_in1k', 'deit_base_patch16_384.fb_in1k', 'deit_small_distilled_patch16_224.fb_in1k', 'deit_small_patch16_224.fb_in1k', 'deit_tiny_distilled_patch16_224.fb_in1k', 'deit_tiny_patch16_224.fb_in1k', 'densenet121.ra_in1k', 'densenet121.tv_in1k', 'densenet161.tv_in1k', 'densenet169.tv_in1k', 'densenet201.tv_in1k', 'densenetblur121d.ra_in1k', 'dla34.in1k', 'dla46_c.in1k', 'dla46x_c.in1k', 'dla60.in1k', 'dla60_res2net.in1k', 'dla60_res2next.in1k', 'dla60x.in1k', 'dla60x_c.in1k', 'dla102.in1k', 'dla102x2.in1k', 'dla102x.in1k', 'dla169.in1k', 'dm_nfnet_f0.dm_in1k', 'dm_nfnet_f1.dm_in1k', 'dm_nfnet_f2.dm_in1k', 'dm_nfnet_f3.dm_in1k', 'dm_nfnet_f4.dm_in1k', 'dm_nfnet_f5.dm_in1k', 'dm_nfnet_f6.dm_in1k', 'dpn68.mx_in1k', 'dpn68b.mx_in1k', 'dpn68b.ra_in1k', 'dpn92.mx_in1k', 'dpn98.mx_in1k', 'dpn107.mx_in1k', 'dpn131.mx_in1k', 'eca_botnext26ts_256.c1_in1k', 'eca_halonext26ts.c1_in1k', 'eca_nfnet_l0.ra2_in1k', 'eca_nfnet_l1.ra2_in1k', 'eca_nfnet_l2.ra3_in1k', 'eca_resnet33ts.ra2_in1k', 'eca_resnext26ts.ch_in1k', 'ecaresnet26t.ra2_in1k', 'ecaresnet50d.miil_in1k', 'ecaresnet50d_pruned.miil_in1k', 'ecaresnet50t.a1_in1k', 'ecaresnet50t.a2_in1k', 'ecaresnet50t.a3_in1k', 'ecaresnet50t.ra2_in1k', 'ecaresnet101d.miil_in1k', 'ecaresnet101d_pruned.miil_in1k', 'ecaresnet269d.ra2_in1k', 'ecaresnetlight.miil_in1k', 'edgenext_base.in21k_ft_in1k', 'edgenext_base.usi_in1k', 'edgenext_small.usi_in1k', 'edgenext_small_rw.sw_in1k', 'edgenext_x_small.in1k', 'edgenext_xx_small.in1k', 'efficientformer_l1.snap_dist_in1k', 'efficientformer_l3.snap_dist_in1k', 'efficientformer_l7.snap_dist_in1k', 'efficientformerv2_l.snap_dist_in1k', 'efficientformerv2_s0.snap_dist_in1k', 'efficientformerv2_s1.snap_dist_in1k', 'efficientformerv2_s2.snap_dist_in1k', 'efficientnet_b0.ra_in1k', 'efficientnet_b1.ft_in1k', 'efficientnet_b1_pruned.in1k', 'efficientnet_b2.ra_in1k', 'efficientnet_b2_pruned.in1k', 'efficientnet_b3.ra2_in1k', 'efficientnet_b3_pruned.in1k', 'efficientnet_b4.ra2_in1k', 'efficientnet_b5.sw_in12k', 'efficientnet_b5.sw_in12k_ft_in1k', 'efficientnet_el.ra_in1k', 'efficientnet_el_pruned.in1k', 'efficientnet_em.ra2_in1k', 'efficientnet_es.ra_in1k', 'efficientnet_es_pruned.in1k', 'efficientnet_lite0.ra_in1k', 'efficientnetv2_rw_m.agc_in1k', 'efficientnetv2_rw_s.ra2_in1k', 'efficientnetv2_rw_t.ra2_in1k', 'efficientvit_b0.r224_in1k', 'efficientvit_b1.r224_in1k', 'efficientvit_b1.r256_in1k', 'efficientvit_b1.r288_in1k', 'efficientvit_b2.r224_in1k', 'efficientvit_b2.r256_in1k', 'efficientvit_b2.r288_in1k', 'efficientvit_b3.r224_in1k', 'efficientvit_b3.r256_in1k', 'efficientvit_b3.r288_in1k', 'efficientvit_m0.r224_in1k', 'efficientvit_m1.r224_in1k', 'efficientvit_m2.r224_in1k', 'efficientvit_m3.r224_in1k', 'efficientvit_m4.r224_in1k', 'efficientvit_m5.r224_in1k', 'ese_vovnet19b_dw.ra_in1k', 'ese_vovnet39b.ra_in1k', 'eva02_base_patch14_224.mim_in22k', 'eva02_base_patch14_448.mim_in22k_ft_in1k', 'eva02_base_patch14_448.mim_in22k_ft_in22k', 'eva02_base_patch14_448.mim_in22k_ft_in22k_in1k', 'eva02_base_patch16_clip_224.merged2b', 'eva02_enormous_patch14_clip_224.laion2b', 'eva02_enormous_patch14_clip_224.laion2b_plus', 'eva02_large_patch14_224.mim_in22k', 'eva02_large_patch14_224.mim_m38m', 'eva02_large_patch14_448.mim_in22k_ft_in1k', 'eva02_large_patch14_448.mim_in22k_ft_in22k', 'eva02_large_patch14_448.mim_in22k_ft_in22k_in1k', 'eva02_large_patch14_448.mim_m38m_ft_in1k', 'eva02_large_patch14_448.mim_m38m_ft_in22k', 'eva02_large_patch14_448.mim_m38m_ft_in22k_in1k', 'eva02_large_patch14_clip_224.merged2b', 'eva02_large_patch14_clip_336.merged2b', 'eva02_small_patch14_224.mim_in22k', 'eva02_small_patch14_336.mim_in22k_ft_in1k', 'eva02_tiny_patch14_224.mim_in22k', 'eva02_tiny_patch14_336.mim_in22k_ft_in1k', 'eva_giant_patch14_224.clip_ft_in1k', 'eva_giant_patch14_336.clip_ft_in1k', 'eva_giant_patch14_336.m30m_ft_in22k_in1k', 'eva_giant_patch14_560.m30m_ft_in22k_in1k', 'eva_giant_patch14_clip_224.laion400m', 'eva_giant_patch14_clip_224.merged2b', 'eva_large_patch14_196.in22k_ft_in1k', 'eva_large_patch14_196.in22k_ft_in22k_in1k', 'eva_large_patch14_336.in22k_ft_in1k', 'eva_large_patch14_336.in22k_ft_in22k_in1k', 'fastvit_ma36.apple_dist_in1k', 'fastvit_ma36.apple_in1k', 'fastvit_s12.apple_dist_in1k', 'fastvit_s12.apple_in1k', 'fastvit_sa12.apple_dist_in1k', 'fastvit_sa12.apple_in1k', 'fastvit_sa24.apple_dist_in1k', 'fastvit_sa24.apple_in1k', 'fastvit_sa36.apple_dist_in1k', 'fastvit_sa36.apple_in1k', 'fastvit_t8.apple_dist_in1k', 'fastvit_t8.apple_in1k', 'fastvit_t12.apple_dist_in1k', 'fastvit_t12.apple_in1k', 'fbnetc_100.rmsp_in1k', 'fbnetv3_b.ra2_in1k', 'fbnetv3_d.ra2_in1k', 'fbnetv3_g.ra2_in1k', 'flexivit_base.300ep_in1k', 'flexivit_base.300ep_in21k', 'flexivit_base.600ep_in1k', 'flexivit_base.1000ep_in21k', 'flexivit_base.1200ep_in1k', 'flexivit_base.patch16_in21k', 'flexivit_base.patch30_in21k', 'flexivit_large.300ep_in1k', 'flexivit_large.600ep_in1k', 'flexivit_large.1200ep_in1k', 'flexivit_small.300ep_in1k', 'flexivit_small.600ep_in1k', 'flexivit_small.1200ep_in1k', 'focalnet_base_lrf.ms_in1k', 'focalnet_base_srf.ms_in1k', 'focalnet_huge_fl3.ms_in22k', 'focalnet_huge_fl4.ms_in22k', 'focalnet_large_fl3.ms_in22k', 'focalnet_large_fl4.ms_in22k', 'focalnet_small_lrf.ms_in1k', 'focalnet_small_srf.ms_in1k', 'focalnet_tiny_lrf.ms_in1k', 'focalnet_tiny_srf.ms_in1k', 'focalnet_xlarge_fl3.ms_in22k', 'focalnet_xlarge_fl4.ms_in22k', 'gc_efficientnetv2_rw_t.agc_in1k', 'gcresnet33ts.ra2_in1k', 'gcresnet50t.ra2_in1k', 'gcresnext26ts.ch_in1k', 'gcresnext50ts.ch_in1k', 'gcvit_base.in1k', 'gcvit_small.in1k', 'gcvit_tiny.in1k', 'gcvit_xtiny.in1k', 'gcvit_xxtiny.in1k', 'gernet_l.idstcv_in1k', 'gernet_m.idstcv_in1k', 'gernet_s.idstcv_in1k', 'ghostnet_100.in1k', 'ghostnetv2_100.in1k', 'ghostnetv2_130.in1k', 'ghostnetv2_160.in1k', 'gmixer_24_224.ra3_in1k', 'gmlp_s16_224.ra3_in1k', 'halo2botnet50ts_256.a1h_in1k', 'halonet26t.a1h_in1k', 'halonet50ts.a1h_in1k', 'haloregnetz_b.ra3_in1k', 'hardcorenas_a.miil_green_in1k', 'hardcorenas_b.miil_green_in1k', 'hardcorenas_c.miil_green_in1k', 'hardcorenas_d.miil_green_in1k', 'hardcorenas_e.miil_green_in1k', 'hardcorenas_f.miil_green_in1k', 'hrnet_w18.ms_aug_in1k', 'hrnet_w18.ms_in1k', 'hrnet_w18_small.gluon_in1k', 'hrnet_w18_small.ms_in1k', 'hrnet_w18_small_v2.gluon_in1k', 'hrnet_w18_small_v2.ms_in1k', 'hrnet_w18_ssld.paddle_in1k', 'hrnet_w30.ms_in1k', 'hrnet_w32.ms_in1k', 'hrnet_w40.ms_in1k', 'hrnet_w44.ms_in1k', 'hrnet_w48.ms_in1k', 'hrnet_w48_ssld.paddle_in1k', 'hrnet_w64.ms_in1k', 'inception_next_base.sail_in1k', 'inception_next_base.sail_in1k_384', 'inception_next_small.sail_in1k', 'inception_next_tiny.sail_in1k', 'inception_resnet_v2.tf_ens_adv_in1k', 'inception_resnet_v2.tf_in1k', 'inception_v3.gluon_in1k', 'inception_v3.tf_adv_in1k', 'inception_v3.tf_in1k', 'inception_v3.tv_in1k', 'inception_v4.tf_in1k', 'lambda_resnet26rpt_256.c1_in1k', 'lambda_resnet26t.c1_in1k', 'lambda_resnet50ts.a1h_in1k', 'lamhalobotnet50ts_256.a1h_in1k', 'lcnet_050.ra2_in1k', 'lcnet_075.ra2_in1k', 'lcnet_100.ra2_in1k', 'legacy_senet154.in1k', 'legacy_seresnet18.in1k', 'legacy_seresnet34.in1k', 'legacy_seresnet50.in1k', 'legacy_seresnet101.in1k', 'legacy_seresnet152.in1k', 'legacy_seresnext26_32x4d.in1k', 'legacy_seresnext50_32x4d.in1k', 'legacy_seresnext101_32x4d.in1k', 'legacy_xception.tf_in1k', 'levit_128.fb_dist_in1k', 'levit_128s.fb_dist_in1k', 'levit_192.fb_dist_in1k', 'levit_256.fb_dist_in1k', 'levit_384.fb_dist_in1k', 'levit_conv_128.fb_dist_in1k', 'levit_conv_128s.fb_dist_in1k', 'levit_conv_192.fb_dist_in1k', 'levit_conv_256.fb_dist_in1k', 'levit_conv_384.fb_dist_in1k', 'maxvit_base_tf_224.in1k', 'maxvit_base_tf_224.in21k', 'maxvit_base_tf_384.in1k', 'maxvit_base_tf_384.in21k_ft_in1k', 'maxvit_base_tf_512.in1k', 'maxvit_base_tf_512.in21k_ft_in1k', 'maxvit_large_tf_224.in1k', 'maxvit_large_tf_224.in21k', 'maxvit_large_tf_384.in1k', 'maxvit_large_tf_384.in21k_ft_in1k', 'maxvit_large_tf_512.in1k', 'maxvit_large_tf_512.in21k_ft_in1k', 'maxvit_nano_rw_256.sw_in1k', 'maxvit_rmlp_base_rw_224.sw_in12k', 'maxvit_rmlp_base_rw_224.sw_in12k_ft_in1k', 'maxvit_rmlp_base_rw_384.sw_in12k_ft_in1k', 'maxvit_rmlp_nano_rw_256.sw_in1k', 'maxvit_rmlp_pico_rw_256.sw_in1k', 'maxvit_rmlp_small_rw_224.sw_in1k', 'maxvit_rmlp_tiny_rw_256.sw_in1k', 'maxvit_small_tf_224.in1k', 'maxvit_small_tf_384.in1k', 'maxvit_small_tf_512.in1k', 'maxvit_tiny_rw_224.sw_in1k', 'maxvit_tiny_tf_224.in1k', 'maxvit_tiny_tf_384.in1k', 'maxvit_tiny_tf_512.in1k', 'maxvit_xlarge_tf_224.in21k', 'maxvit_xlarge_tf_384.in21k_ft_in1k', 'maxvit_xlarge_tf_512.in21k_ft_in1k', 'maxxvit_rmlp_nano_rw_256.sw_in1k', 'maxxvit_rmlp_small_rw_256.sw_in1k', 'maxxvitv2_nano_rw_256.sw_in1k', 'maxxvitv2_rmlp_base_rw_224.sw_in12k', 'maxxvitv2_rmlp_base_rw_224.sw_in12k_ft_in1k', 'maxxvitv2_rmlp_base_rw_384.sw_in12k_ft_in1k', 'mixer_b16_224.goog_in21k', 'mixer_b16_224.goog_in21k_ft_in1k', 'mixer_b16_224.miil_in21k', 'mixer_b16_224.miil_in21k_ft_in1k', 'mixer_l16_224.goog_in21k', 'mixer_l16_224.goog_in21k_ft_in1k', 'mixnet_l.ft_in1k', 'mixnet_m.ft_in1k', 'mixnet_s.ft_in1k', 'mixnet_xl.ra_in1k', 'mnasnet_100.rmsp_in1k', 'mnasnet_small.lamb_in1k', 'mobilenetv2_050.lamb_in1k', 'mobilenetv2_100.ra_in1k', 'mobilenetv2_110d.ra_in1k', 'mobilenetv2_120d.ra_in1k', 'mobilenetv2_140.ra_in1k', 'mobilenetv3_large_100.miil_in21k', 'mobilenetv3_large_100.miil_in21k_ft_in1k', 'mobilenetv3_large_100.ra_in1k', 'mobilenetv3_rw.rmsp_in1k', 'mobilenetv3_small_050.lamb_in1k', 'mobilenetv3_small_075.lamb_in1k', 'mobilenetv3_small_100.lamb_in1k', 'mobileone_s0.apple_in1k', 'mobileone_s1.apple_in1k', 'mobileone_s2.apple_in1k', 'mobileone_s3.apple_in1k', 'mobileone_s4.apple_in1k', 'mobilevit_s.cvnets_in1k', 'mobilevit_xs.cvnets_in1k', 'mobilevit_xxs.cvnets_in1k', 'mobilevitv2_050.cvnets_in1k', 'mobilevitv2_075.cvnets_in1k', 'mobilevitv2_100.cvnets_in1k', 'mobilevitv2_125.cvnets_in1k', 'mobilevitv2_150.cvnets_in1k', 'mobilevitv2_150.cvnets_in22k_ft_in1k', 'mobilevitv2_150.cvnets_in22k_ft_in1k_384', 'mobilevitv2_175.cvnets_in1k', 'mobilevitv2_175.cvnets_in22k_ft_in1k', 'mobilevitv2_175.cvnets_in22k_ft_in1k_384', 'mobilevitv2_200.cvnets_in1k', 'mobilevitv2_200.cvnets_in22k_ft_in1k', 'mobilevitv2_200.cvnets_in22k_ft_in1k_384', 'mvitv2_base.fb_in1k', 'mvitv2_base_cls.fb_inw21k', 'mvitv2_huge_cls.fb_inw21k', 'mvitv2_large.fb_in1k', 'mvitv2_large_cls.fb_inw21k', 'mvitv2_small.fb_in1k', 'mvitv2_tiny.fb_in1k', 'nasnetalarge.tf_in1k', 'nest_base_jx.goog_in1k', 'nest_small_jx.goog_in1k', 'nest_tiny_jx.goog_in1k', 'nf_regnet_b1.ra2_in1k', 'nf_resnet50.ra2_in1k', 'nfnet_l0.ra2_in1k', 'pit_b_224.in1k', 'pit_b_distilled_224.in1k', 'pit_s_224.in1k', 'pit_s_distilled_224.in1k', 'pit_ti_224.in1k', 'pit_ti_distilled_224.in1k', 'pit_xs_224.in1k', 'pit_xs_distilled_224.in1k', 'pnasnet5large.tf_in1k', 'poolformer_m36.sail_in1k', 'poolformer_m48.sail_in1k', 'poolformer_s12.sail_in1k', 'poolformer_s24.sail_in1k', 'poolformer_s36.sail_in1k', 'poolformerv2_m36.sail_in1k', 'poolformerv2_m48.sail_in1k', 'poolformerv2_s12.sail_in1k', 'poolformerv2_s24.sail_in1k', 'poolformerv2_s36.sail_in1k', 'pvt_v2_b0.in1k', 'pvt_v2_b1.in1k', 'pvt_v2_b2.in1k', 'pvt_v2_b2_li.in1k', 'pvt_v2_b3.in1k', 'pvt_v2_b4.in1k', 'pvt_v2_b5.in1k', 'regnetv_040.ra3_in1k', 'regnetv_064.ra3_in1k', 'regnetx_002.pycls_in1k', 'regnetx_004.pycls_in1k', 'regnetx_004_tv.tv2_in1k', 'regnetx_006.pycls_in1k', 'regnetx_008.pycls_in1k', 'regnetx_008.tv2_in1k', 'regnetx_016.pycls_in1k', 'regnetx_016.tv2_in1k', 'regnetx_032.pycls_in1k', 'regnetx_032.tv2_in1k', 'regnetx_040.pycls_in1k', 'regnetx_064.pycls_in1k', 'regnetx_080.pycls_in1k', 'regnetx_080.tv2_in1k', 'regnetx_120.pycls_in1k', 'regnetx_160.pycls_in1k', 'regnetx_160.tv2_in1k', 'regnetx_320.pycls_in1k', 'regnetx_320.tv2_in1k', 'regnety_002.pycls_in1k', 'regnety_004.pycls_in1k', 'regnety_004.tv2_in1k', 'regnety_006.pycls_in1k', 'regnety_008.pycls_in1k', 'regnety_008_tv.tv2_in1k', 'regnety_016.pycls_in1k', 'regnety_016.tv2_in1k', 'regnety_032.pycls_in1k', 'regnety_032.ra_in1k', 'regnety_032.tv2_in1k', 'regnety_040.pycls_in1k', 'regnety_040.ra3_in1k', 'regnety_064.pycls_in1k', 'regnety_064.ra3_in1k', 'regnety_080.pycls_in1k', 'regnety_080.ra3_in1k', 'regnety_080_tv.tv2_in1k', 'regnety_120.pycls_in1k', 'regnety_120.sw_in12k', 'regnety_120.sw_in12k_ft_in1k', 'regnety_160.deit_in1k', 'regnety_160.lion_in12k_ft_in1k', 'regnety_160.pycls_in1k', 'regnety_160.sw_in12k', 'regnety_160.sw_in12k_ft_in1k', 'regnety_160.swag_ft_in1k', 'regnety_160.swag_lc_in1k', 'regnety_160.tv2_in1k', 'regnety_320.pycls_in1k', 'regnety_320.seer', 'regnety_320.seer_ft_in1k', 'regnety_320.swag_ft_in1k', 'regnety_320.swag_lc_in1k', 'regnety_320.tv2_in1k', 'regnety_640.seer', 'regnety_640.seer_ft_in1k', 'regnety_1280.seer', 'regnety_1280.seer_ft_in1k', 'regnety_1280.swag_ft_in1k', 'regnety_1280.swag_lc_in1k', 'regnety_2560.seer_ft_in1k', 'regnetz_040.ra3_in1k', 'regnetz_040_h.ra3_in1k', 'regnetz_b16.ra3_in1k', 'regnetz_c16.ra3_in1k', 'regnetz_c16_evos.ch_in1k', 'regnetz_d8.ra3_in1k', 'regnetz_d8_evos.ch_in1k', 'regnetz_d32.ra3_in1k', 'regnetz_e8.ra3_in1k', 'repghostnet_050.in1k', 'repghostnet_058.in1k', 'repghostnet_080.in1k', 'repghostnet_100.in1k', 'repghostnet_111.in1k', 'repghostnet_130.in1k', 'repghostnet_150.in1k', 'repghostnet_200.in1k', 'repvgg_a0.rvgg_in1k', 'repvgg_a1.rvgg_in1k', 'repvgg_a2.rvgg_in1k', 'repvgg_b0.rvgg_in1k', 'repvgg_b1.rvgg_in1k', 'repvgg_b1g4.rvgg_in1k', 'repvgg_b2.rvgg_in1k', 'repvgg_b2g4.rvgg_in1k', 'repvgg_b3.rvgg_in1k', 'repvgg_b3g4.rvgg_in1k', 'repvgg_d2se.rvgg_in1k', 'repvit_m0_9.dist_300e_in1k', 'repvit_m0_9.dist_450e_in1k', 'repvit_m1.dist_in1k', 'repvit_m1_0.dist_300e_in1k', 'repvit_m1_0.dist_450e_in1k', 'repvit_m1_1.dist_300e_in1k', 'repvit_m1_1.dist_450e_in1k', 'repvit_m1_5.dist_300e_in1k', 'repvit_m1_5.dist_450e_in1k', 'repvit_m2.dist_in1k', 'repvit_m2_3.dist_300e_in1k', 'repvit_m2_3.dist_450e_in1k', 'repvit_m3.dist_in1k', 'res2net50_14w_8s.in1k', 'res2net50_26w_4s.in1k', 'res2net50_26w_6s.in1k', 'res2net50_26w_8s.in1k', 'res2net50_48w_2s.in1k', 'res2net50d.in1k', 'res2net101_26w_4s.in1k', 'res2net101d.in1k', 'res2next50.in1k', 'resmlp_12_224.fb_dino', 'resmlp_12_224.fb_distilled_in1k', 'resmlp_12_224.fb_in1k', 'resmlp_24_224.fb_dino', 'resmlp_24_224.fb_distilled_in1k', 'resmlp_24_224.fb_in1k', 'resmlp_36_224.fb_distilled_in1k', 'resmlp_36_224.fb_in1k', 'resmlp_big_24_224.fb_distilled_in1k', 'resmlp_big_24_224.fb_in1k', 'resmlp_big_24_224.fb_in22k_ft_in1k', 'resnest14d.gluon_in1k', 'resnest26d.gluon_in1k', 'resnest50d.in1k', 'resnest50d_1s4x24d.in1k', 'resnest50d_4s2x40d.in1k', 'resnest101e.in1k', 'resnest200e.in1k', 'resnest269e.in1k', 'resnet10t.c3_in1k', 'resnet14t.c3_in1k', 'resnet18.a1_in1k', 'resnet18.a2_in1k', 'resnet18.a3_in1k', 'resnet18.fb_ssl_yfcc100m_ft_in1k', 'resnet18.fb_swsl_ig1b_ft_in1k', 'resnet18.gluon_in1k', 'resnet18.tv_in1k', 'resnet18d.ra2_in1k', 'resnet26.bt_in1k', 'resnet26d.bt_in1k', 'resnet26t.ra2_in1k', 'resnet32ts.ra2_in1k', 'resnet33ts.ra2_in1k', 'resnet34.a1_in1k', 'resnet34.a2_in1k', 'resnet34.a3_in1k', 'resnet34.bt_in1k', 'resnet34.gluon_in1k', 'resnet34.tv_in1k', 'resnet34d.ra2_in1k', 'resnet50.a1_in1k', 'resnet50.a1h_in1k', 'resnet50.a2_in1k', 'resnet50.a3_in1k', 'resnet50.am_in1k', 'resnet50.b1k_in1k', 'resnet50.b2k_in1k', 'resnet50.bt_in1k', 'resnet50.c1_in1k', 'resnet50.c2_in1k', 'resnet50.d_in1k', 'resnet50.fb_ssl_yfcc100m_ft_in1k', 'resnet50.fb_swsl_ig1b_ft_in1k', 'resnet50.gluon_in1k', 'resnet50.ra_in1k', 'resnet50.ram_in1k', 'resnet50.tv2_in1k', 'resnet50.tv_in1k', 'resnet50_gn.a1h_in1k', 'resnet50c.gluon_in1k', 'resnet50d.a1_in1k', 'resnet50d.a2_in1k', 'resnet50d.a3_in1k', 'resnet50d.gluon_in1k', 'resnet50d.ra2_in1k', 'resnet50s.gluon_in1k', 'resnet51q.ra2_in1k', 'resnet61q.ra2_in1k', 'resnet101.a1_in1k', 'resnet101.a1h_in1k', 'resnet101.a2_in1k', 'resnet101.a3_in1k', 'resnet101.gluon_in1k', 'resnet101.tv2_in1k', 'resnet101.tv_in1k', 'resnet101c.gluon_in1k', 'resnet101d.gluon_in1k', 'resnet101d.ra2_in1k', 'resnet101s.gluon_in1k', 'resnet152.a1_in1k', 'resnet152.a1h_in1k', 'resnet152.a2_in1k', 'resnet152.a3_in1k', 'resnet152.gluon_in1k', 'resnet152.tv2_in1k', 'resnet152.tv_in1k', 'resnet152c.gluon_in1k', 'resnet152d.gluon_in1k', 'resnet152d.ra2_in1k', 'resnet152s.gluon_in1k', 'resnet200d.ra2_in1k', 'resnetaa50.a1h_in1k', 'resnetaa50d.d_in12k', 'resnetaa50d.sw_in12k', 'resnetaa50d.sw_in12k_ft_in1k', 'resnetaa101d.sw_in12k', 'resnetaa101d.sw_in12k_ft_in1k', 'resnetblur50.bt_in1k', 'resnetrs50.tf_in1k', 'resnetrs101.tf_in1k', 'resnetrs152.tf_in1k', 'resnetrs200.tf_in1k', 'resnetrs270.tf_in1k', 'resnetrs350.tf_in1k', 'resnetrs420.tf_in1k', 'resnetv2_50.a1h_in1k', 'resnetv2_50d_evos.ah_in1k', 'resnetv2_50d_gn.ah_in1k', 'resnetv2_50x1_bit.goog_distilled_in1k', 'resnetv2_50x1_bit.goog_in21k', 'resnetv2_50x1_bit.goog_in21k_ft_in1k', 'resnetv2_50x3_bit.goog_in21k', 'resnetv2_50x3_bit.goog_in21k_ft_in1k', 'resnetv2_101.a1h_in1k', 'resnetv2_101x1_bit.goog_in21k', 'resnetv2_101x1_bit.goog_in21k_ft_in1k', 'resnetv2_101x3_bit.goog_in21k', 'resnetv2_101x3_bit.goog_in21k_ft_in1k', 'resnetv2_152x2_bit.goog_in21k', 'resnetv2_152x2_bit.goog_in21k_ft_in1k', 'resnetv2_152x2_bit.goog_teacher_in21k_ft_in1k', 'resnetv2_152x2_bit.goog_teacher_in21k_ft_in1k_384', 'resnetv2_152x4_bit.goog_in21k', 'resnetv2_152x4_bit.goog_in21k_ft_in1k', 'resnext26ts.ra2_in1k', 'resnext50_32x4d.a1_in1k', 'resnext50_32x4d.a1h_in1k', 'resnext50_32x4d.a2_in1k', 'resnext50_32x4d.a3_in1k', 'resnext50_32x4d.fb_ssl_yfcc100m_ft_in1k', 'resnext50_32x4d.fb_swsl_ig1b_ft_in1k', 'resnext50_32x4d.gluon_in1k', 'resnext50_32x4d.ra_in1k', 'resnext50_32x4d.tv2_in1k', 'resnext50_32x4d.tv_in1k', 'resnext50d_32x4d.bt_in1k', 'resnext101_32x4d.fb_ssl_yfcc100m_ft_in1k', 'resnext101_32x4d.fb_swsl_ig1b_ft_in1k', 'resnext101_32x4d.gluon_in1k', 'resnext101_32x8d.fb_ssl_yfcc100m_ft_in1k', 'resnext101_32x8d.fb_swsl_ig1b_ft_in1k', 'resnext101_32x8d.fb_wsl_ig1b_ft_in1k', 'resnext101_32x8d.tv2_in1k', 'resnext101_32x8d.tv_in1k', 'resnext101_32x16d.fb_ssl_yfcc100m_ft_in1k', 'resnext101_32x16d.fb_swsl_ig1b_ft_in1k', 'resnext101_32x16d.fb_wsl_ig1b_ft_in1k', 'resnext101_32x32d.fb_wsl_ig1b_ft_in1k', 'resnext101_64x4d.c1_in1k', 'resnext101_64x4d.gluon_in1k', 'resnext101_64x4d.tv_in1k', 'rexnet_100.nav_in1k', 'rexnet_130.nav_in1k', 'rexnet_150.nav_in1k', 'rexnet_200.nav_in1k', 'rexnet_300.nav_in1k', 'rexnetr_200.sw_in12k', 'rexnetr_200.sw_in12k_ft_in1k', 'rexnetr_300.sw_in12k', 'rexnetr_300.sw_in12k_ft_in1k', 'samvit_base_patch16.sa1b', 'samvit_huge_patch16.sa1b', 'samvit_large_patch16.sa1b', 'sebotnet33ts_256.a1h_in1k', 'sehalonet33ts.ra2_in1k', 'selecsls42b.in1k', 'selecsls60.in1k', 'selecsls60b.in1k', 'semnasnet_075.rmsp_in1k', 'semnasnet_100.rmsp_in1k', 'senet154.gluon_in1k', 'sequencer2d_l.in1k', 'sequencer2d_m.in1k', 'sequencer2d_s.in1k', 'seresnet33ts.ra2_in1k', 'seresnet50.a1_in1k', 'seresnet50.a2_in1k', 'seresnet50.a3_in1k', 'seresnet50.ra2_in1k', 'seresnet152d.ra2_in1k', 'seresnext26d_32x4d.bt_in1k', 'seresnext26t_32x4d.bt_in1k', 'seresnext26ts.ch_in1k', 'seresnext50_32x4d.gluon_in1k', 'seresnext50_32x4d.racm_in1k', 'seresnext101_32x4d.gluon_in1k', 'seresnext101_32x8d.ah_in1k', 'seresnext101_64x4d.gluon_in1k', 'seresnext101d_32x8d.ah_in1k', 'seresnextaa101d_32x8d.ah_in1k', 'seresnextaa101d_32x8d.sw_in12k', 'seresnextaa101d_32x8d.sw_in12k_ft_in1k', 'seresnextaa101d_32x8d.sw_in12k_ft_in1k_288', 'seresnextaa201d_32x8d.sw_in12k', 'seresnextaa201d_32x8d.sw_in12k_ft_in1k_384', 'skresnet18.ra_in1k', 'skresnet34.ra_in1k', 'skresnext50_32x4d.ra_in1k', 'spnasnet_100.rmsp_in1k', 'swin_base_patch4_window7_224.ms_in1k', 'swin_base_patch4_window7_224.ms_in22k', 'swin_base_patch4_window7_224.ms_in22k_ft_in1k', 'swin_base_patch4_window12_384.ms_in1k', 'swin_base_patch4_window12_384.ms_in22k', 'swin_base_patch4_window12_384.ms_in22k_ft_in1k', 'swin_large_patch4_window7_224.ms_in22k', 'swin_large_patch4_window7_224.ms_in22k_ft_in1k', 'swin_large_patch4_window12_384.ms_in22k', 'swin_large_patch4_window12_384.ms_in22k_ft_in1k', 'swin_s3_base_224.ms_in1k', 'swin_s3_small_224.ms_in1k', 'swin_s3_tiny_224.ms_in1k', 'swin_small_patch4_window7_224.ms_in1k', 'swin_small_patch4_window7_224.ms_in22k', 'swin_small_patch4_window7_224.ms_in22k_ft_in1k', 'swin_tiny_patch4_window7_224.ms_in1k', 'swin_tiny_patch4_window7_224.ms_in22k', 'swin_tiny_patch4_window7_224.ms_in22k_ft_in1k', 'swinv2_base_window8_256.ms_in1k', 'swinv2_base_window12_192.ms_in22k', 'swinv2_base_window12to16_192to256.ms_in22k_ft_in1k', 'swinv2_base_window12to24_192to384.ms_in22k_ft_in1k', 'swinv2_base_window16_256.ms_in1k', 'swinv2_cr_small_224.sw_in1k', 'swinv2_cr_small_ns_224.sw_in1k', 'swinv2_cr_tiny_ns_224.sw_in1k', 'swinv2_large_window12_192.ms_in22k', 'swinv2_large_window12to16_192to256.ms_in22k_ft_in1k', 'swinv2_large_window12to24_192to384.ms_in22k_ft_in1k', 'swinv2_small_window8_256.ms_in1k', 'swinv2_small_window16_256.ms_in1k', 'swinv2_tiny_window8_256.ms_in1k', 'swinv2_tiny_window16_256.ms_in1k', 'tf_efficientnet_b0.aa_in1k', 'tf_efficientnet_b0.ap_in1k', 'tf_efficientnet_b0.in1k', 'tf_efficientnet_b0.ns_jft_in1k', 'tf_efficientnet_b1.aa_in1k', 'tf_efficientnet_b1.ap_in1k', 'tf_efficientnet_b1.in1k', 'tf_efficientnet_b1.ns_jft_in1k', 'tf_efficientnet_b2.aa_in1k', 'tf_efficientnet_b2.ap_in1k', 'tf_efficientnet_b2.in1k', 'tf_efficientnet_b2.ns_jft_in1k', 'tf_efficientnet_b3.aa_in1k', 'tf_efficientnet_b3.ap_in1k', 'tf_efficientnet_b3.in1k', 'tf_efficientnet_b3.ns_jft_in1k', 'tf_efficientnet_b4.aa_in1k', 'tf_efficientnet_b4.ap_in1k', 'tf_efficientnet_b4.in1k', 'tf_efficientnet_b4.ns_jft_in1k', 'tf_efficientnet_b5.aa_in1k', 'tf_efficientnet_b5.ap_in1k', 'tf_efficientnet_b5.in1k', 'tf_efficientnet_b5.ns_jft_in1k', 'tf_efficientnet_b5.ra_in1k', 'tf_efficientnet_b6.aa_in1k', 'tf_efficientnet_b6.ap_in1k', 'tf_efficientnet_b6.ns_jft_in1k', 'tf_efficientnet_b7.aa_in1k', 'tf_efficientnet_b7.ap_in1k', 'tf_efficientnet_b7.ns_jft_in1k', 'tf_efficientnet_b7.ra_in1k', 'tf_efficientnet_b8.ap_in1k', 'tf_efficientnet_b8.ra_in1k', 'tf_efficientnet_cc_b0_4e.in1k', 'tf_efficientnet_cc_b0_8e.in1k', 'tf_efficientnet_cc_b1_8e.in1k', 'tf_efficientnet_el.in1k', 'tf_efficientnet_em.in1k', 'tf_efficientnet_es.in1k', 'tf_efficientnet_l2.ns_jft_in1k', 'tf_efficientnet_l2.ns_jft_in1k_475', 'tf_efficientnet_lite0.in1k', 'tf_efficientnet_lite1.in1k', 'tf_efficientnet_lite2.in1k', 'tf_efficientnet_lite3.in1k', 'tf_efficientnet_lite4.in1k', 'tf_efficientnetv2_b0.in1k', 'tf_efficientnetv2_b1.in1k', 'tf_efficientnetv2_b2.in1k', 'tf_efficientnetv2_b3.in1k', 'tf_efficientnetv2_b3.in21k', 'tf_efficientnetv2_b3.in21k_ft_in1k', 'tf_efficientnetv2_l.in1k', 'tf_efficientnetv2_l.in21k', 'tf_efficientnetv2_l.in21k_ft_in1k', 'tf_efficientnetv2_m.in1k', 'tf_efficientnetv2_m.in21k', 'tf_efficientnetv2_m.in21k_ft_in1k', 'tf_efficientnetv2_s.in1k', 'tf_efficientnetv2_s.in21k', 'tf_efficientnetv2_s.in21k_ft_in1k', 'tf_efficientnetv2_xl.in21k', 'tf_efficientnetv2_xl.in21k_ft_in1k', 'tf_mixnet_l.in1k', 'tf_mixnet_m.in1k', 'tf_mixnet_s.in1k', 'tf_mobilenetv3_large_075.in1k', 'tf_mobilenetv3_large_100.in1k', 'tf_mobilenetv3_large_minimal_100.in1k', 'tf_mobilenetv3_small_075.in1k', 'tf_mobilenetv3_small_100.in1k', 'tf_mobilenetv3_small_minimal_100.in1k', 'tiny_vit_5m_224.dist_in22k', 'tiny_vit_5m_224.dist_in22k_ft_in1k', 'tiny_vit_5m_224.in1k', 'tiny_vit_11m_224.dist_in22k', 'tiny_vit_11m_224.dist_in22k_ft_in1k', 'tiny_vit_11m_224.in1k', 'tiny_vit_21m_224.dist_in22k', 'tiny_vit_21m_224.dist_in22k_ft_in1k', 'tiny_vit_21m_224.in1k', 'tiny_vit_21m_384.dist_in22k_ft_in1k', 'tiny_vit_21m_512.dist_in22k_ft_in1k', 'tinynet_a.in1k', 'tinynet_b.in1k', 'tinynet_c.in1k', 'tinynet_d.in1k', 'tinynet_e.in1k', 'tnt_s_patch16_224', 'tresnet_l.miil_in1k', 'tresnet_l.miil_in1k_448', 'tresnet_m.miil_in1k', 'tresnet_m.miil_in1k_448', 'tresnet_m.miil_in21k', 'tresnet_m.miil_in21k_ft_in1k', 'tresnet_v2_l.miil_in21k', 'tresnet_v2_l.miil_in21k_ft_in1k', 'tresnet_xl.miil_in1k', 'tresnet_xl.miil_in1k_448', 'twins_pcpvt_base.in1k', 'twins_pcpvt_large.in1k', 'twins_pcpvt_small.in1k', 'twins_svt_base.in1k', 'twins_svt_large.in1k', 'twins_svt_small.in1k', 'vgg11.tv_in1k', 'vgg11_bn.tv_in1k', 'vgg13.tv_in1k', 'vgg13_bn.tv_in1k', 'vgg16.tv_in1k', 'vgg16_bn.tv_in1k', 'vgg19.tv_in1k', 'vgg19_bn.tv_in1k', 'visformer_small.in1k', 'visformer_tiny.in1k', 'vit_base_patch8_224.augreg2_in21k_ft_in1k', 'vit_base_patch8_224.augreg_in21k', 'vit_base_patch8_224.augreg_in21k_ft_in1k', 'vit_base_patch8_224.dino', 'vit_base_patch14_dinov2.lvd142m', 'vit_base_patch14_reg4_dinov2.lvd142m', 'vit_base_patch16_224.augreg2_in21k_ft_in1k', 'vit_base_patch16_224.augreg_in1k', 'vit_base_patch16_224.augreg_in21k', 'vit_base_patch16_224.augreg_in21k_ft_in1k', 'vit_base_patch16_224.dino', 'vit_base_patch16_224.mae', 'vit_base_patch16_224.orig_in21k_ft_in1k', 'vit_base_patch16_224.sam_in1k', 'vit_base_patch16_224_miil.in21k', 'vit_base_patch16_224_miil.in21k_ft_in1k', 'vit_base_patch16_384.augreg_in1k', 'vit_base_patch16_384.augreg_in21k_ft_in1k', 'vit_base_patch16_384.orig_in21k_ft_in1k', 'vit_base_patch16_clip_224.datacompxl', 'vit_base_patch16_clip_224.dfn2b', 'vit_base_patch16_clip_224.laion2b', 'vit_base_patch16_clip_224.laion2b_ft_in1k', 'vit_base_patch16_clip_224.laion2b_ft_in12k', 'vit_base_patch16_clip_224.laion2b_ft_in12k_in1k', 'vit_base_patch16_clip_224.metaclip_2pt5b', 'vit_base_patch16_clip_224.openai', 'vit_base_patch16_clip_224.openai_ft_in1k', 'vit_base_patch16_clip_224.openai_ft_in12k', 'vit_base_patch16_clip_224.openai_ft_in12k_in1k', 'vit_base_patch16_clip_384.laion2b_ft_in1k', 'vit_base_patch16_clip_384.laion2b_ft_in12k_in1k', 'vit_base_patch16_clip_384.openai_ft_in1k', 'vit_base_patch16_clip_384.openai_ft_in12k_in1k', 'vit_base_patch16_clip_quickgelu_224.metaclip_2pt5b', 'vit_base_patch16_clip_quickgelu_224.openai', 'vit_base_patch16_rpn_224.sw_in1k', 'vit_base_patch16_siglip_224.webli', 'vit_base_patch16_siglip_256.webli', 'vit_base_patch16_siglip_384.webli', 'vit_base_patch16_siglip_512.webli', 'vit_base_patch32_224.augreg_in1k', 'vit_base_patch32_224.augreg_in21k', 'vit_base_patch32_224.augreg_in21k_ft_in1k', 'vit_base_patch32_224.sam_in1k', 'vit_base_patch32_384.augreg_in1k', 'vit_base_patch32_384.augreg_in21k_ft_in1k', 'vit_base_patch32_clip_224.datacompxl', 'vit_base_patch32_clip_224.laion2b', 'vit_base_patch32_clip_224.laion2b_ft_in1k', 'vit_base_patch32_clip_224.laion2b_ft_in12k_in1k', 'vit_base_patch32_clip_224.metaclip_2pt5b', 'vit_base_patch32_clip_224.openai', 'vit_base_patch32_clip_224.openai_ft_in1k', 'vit_base_patch32_clip_256.datacompxl', 'vit_base_patch32_clip_384.laion2b_ft_in12k_in1k', 'vit_base_patch32_clip_384.openai_ft_in12k_in1k', 'vit_base_patch32_clip_448.laion2b_ft_in12k_in1k', 'vit_base_patch32_clip_quickgelu_224.metaclip_2pt5b', 'vit_base_patch32_clip_quickgelu_224.openai', 'vit_base_r50_s16_224.orig_in21k', 'vit_base_r50_s16_384.orig_in21k_ft_in1k', 'vit_giant_patch14_clip_224.laion2b', 'vit_giant_patch14_dinov2.lvd142m', 'vit_giant_patch14_reg4_dinov2.lvd142m', 'vit_giant_patch16_gap_224.in22k_ijepa', 'vit_gigantic_patch14_clip_224.laion2b', 'vit_huge_patch14_224.mae', 'vit_huge_patch14_224.orig_in21k', 'vit_huge_patch14_clip_224.dfn5b', 'vit_huge_patch14_clip_224.laion2b', 'vit_huge_patch14_clip_224.laion2b_ft_in1k', 'vit_huge_patch14_clip_224.laion2b_ft_in12k', 'vit_huge_patch14_clip_224.laion2b_ft_in12k_in1k', 'vit_huge_patch14_clip_224.metaclip_2pt5b', 'vit_huge_patch14_clip_336.laion2b_ft_in12k_in1k', 'vit_huge_patch14_clip_378.dfn5b', 'vit_huge_patch14_clip_quickgelu_224.dfn5b', 'vit_huge_patch14_clip_quickgelu_224.metaclip_2pt5b', 'vit_huge_patch14_clip_quickgelu_378.dfn5b', 'vit_huge_patch14_gap_224.in1k_ijepa', 'vit_huge_patch14_gap_224.in22k_ijepa', 'vit_huge_patch16_gap_448.in1k_ijepa', 'vit_large_patch14_clip_224.datacompxl', 'vit_large_patch14_clip_224.dfn2b', 'vit_large_patch14_clip_224.laion2b', 'vit_large_patch14_clip_224.laion2b_ft_in1k', 'vit_large_patch14_clip_224.laion2b_ft_in12k', 'vit_large_patch14_clip_224.laion2b_ft_in12k_in1k', 'vit_large_patch14_clip_224.metaclip_2pt5b', 'vit_large_patch14_clip_224.openai', 'vit_large_patch14_clip_224.openai_ft_in1k', 'vit_large_patch14_clip_224.openai_ft_in12k', 'vit_large_patch14_clip_224.openai_ft_in12k_in1k', 'vit_large_patch14_clip_336.laion2b_ft_in1k', 'vit_large_patch14_clip_336.laion2b_ft_in12k_in1k', 'vit_large_patch14_clip_336.openai', 'vit_large_patch14_clip_336.openai_ft_in12k_in1k', 'vit_large_patch14_clip_quickgelu_224.dfn2b', 'vit_large_patch14_clip_quickgelu_224.metaclip_2pt5b', 'vit_large_patch14_clip_quickgelu_224.openai', 'vit_large_patch14_clip_quickgelu_336.openai', 'vit_large_patch14_dinov2.lvd142m', 'vit_large_patch14_reg4_dinov2.lvd142m', 'vit_large_patch16_224.augreg_in21k', 'vit_large_patch16_224.augreg_in21k_ft_in1k', 'vit_large_patch16_224.mae', 'vit_large_patch16_384.augreg_in21k_ft_in1k', 'vit_large_patch16_siglip_256.webli', 'vit_large_patch16_siglip_384.webli', 'vit_large_patch32_224.orig_in21k', 'vit_large_patch32_384.orig_in21k_ft_in1k', 'vit_large_r50_s32_224.augreg_in21k', 'vit_large_r50_s32_224.augreg_in21k_ft_in1k', 'vit_large_r50_s32_384.augreg_in21k_ft_in1k', 'vit_medium_patch16_gap_240.sw_in12k', 'vit_medium_patch16_gap_256.sw_in12k_ft_in1k', 'vit_medium_patch16_gap_384.sw_in12k_ft_in1k', 'vit_relpos_base_patch16_224.sw_in1k', 'vit_relpos_base_patch16_clsgap_224.sw_in1k', 'vit_relpos_base_patch32_plus_rpn_256.sw_in1k', 'vit_relpos_medium_patch16_224.sw_in1k', 'vit_relpos_medium_patch16_cls_224.sw_in1k', 'vit_relpos_medium_patch16_rpn_224.sw_in1k', 'vit_relpos_small_patch16_224.sw_in1k', 'vit_small_patch8_224.dino', 'vit_small_patch14_dinov2.lvd142m', 'vit_small_patch14_reg4_dinov2.lvd142m', 'vit_small_patch16_224.augreg_in1k', 'vit_small_patch16_224.augreg_in21k', 'vit_small_patch16_224.augreg_in21k_ft_in1k', 'vit_small_patch16_224.dino', 'vit_small_patch16_384.augreg_in1k', 'vit_small_patch16_384.augreg_in21k_ft_in1k', 'vit_small_patch32_224.augreg_in21k', 'vit_small_patch32_224.augreg_in21k_ft_in1k', 'vit_small_patch32_384.augreg_in21k_ft_in1k', 'vit_small_r26_s32_224.augreg_in21k', 'vit_small_r26_s32_224.augreg_in21k_ft_in1k', 'vit_small_r26_s32_384.augreg_in21k_ft_in1k', 'vit_so400m_patch14_siglip_224.webli', 'vit_so400m_patch14_siglip_384.webli', 'vit_srelpos_medium_patch16_224.sw_in1k', 'vit_srelpos_small_patch16_224.sw_in1k', 'vit_tiny_patch16_224.augreg_in21k', 'vit_tiny_patch16_224.augreg_in21k_ft_in1k', 'vit_tiny_patch16_384.augreg_in21k_ft_in1k', 'vit_tiny_r_s16_p8_224.augreg_in21k', 'vit_tiny_r_s16_p8_224.augreg_in21k_ft_in1k', 'vit_tiny_r_s16_p8_384.augreg_in21k_ft_in1k', 'volo_d1_224.sail_in1k', 'volo_d1_384.sail_in1k', 'volo_d2_224.sail_in1k', 'volo_d2_384.sail_in1k', 'volo_d3_224.sail_in1k', 'volo_d3_448.sail_in1k', 'volo_d4_224.sail_in1k', 'volo_d4_448.sail_in1k', 'volo_d5_224.sail_in1k', 'volo_d5_448.sail_in1k', 'volo_d5_512.sail_in1k', 'wide_resnet50_2.racm_in1k', 'wide_resnet50_2.tv2_in1k', 'wide_resnet50_2.tv_in1k', 'wide_resnet101_2.tv2_in1k', 'wide_resnet101_2.tv_in1k', 'xception41.tf_in1k', 'xception41p.ra3_in1k', 'xception65.ra3_in1k', 'xception65.tf_in1k', 'xception65p.ra3_in1k', 'xception71.tf_in1k', 'xcit_large_24_p8_224.fb_dist_in1k', 'xcit_large_24_p8_224.fb_in1k', 'xcit_large_24_p8_384.fb_dist_in1k', 'xcit_large_24_p16_224.fb_dist_in1k', 'xcit_large_24_p16_224.fb_in1k', 'xcit_large_24_p16_384.fb_dist_in1k', 'xcit_medium_24_p8_224.fb_dist_in1k', 'xcit_medium_24_p8_224.fb_in1k', 'xcit_medium_24_p8_384.fb_dist_in1k', 'xcit_medium_24_p16_224.fb_dist_in1k', 'xcit_medium_24_p16_224.fb_in1k', 'xcit_medium_24_p16_384.fb_dist_in1k', 'xcit_nano_12_p8_224.fb_dist_in1k', 'xcit_nano_12_p8_224.fb_in1k', 'xcit_nano_12_p8_384.fb_dist_in1k', 'xcit_nano_12_p16_224.fb_dist_in1k', 'xcit_nano_12_p16_224.fb_in1k', 'xcit_nano_12_p16_384.fb_dist_in1k', 'xcit_small_12_p8_224.fb_dist_in1k', 'xcit_small_12_p8_224.fb_in1k', 'xcit_small_12_p8_384.fb_dist_in1k', 'xcit_small_12_p16_224.fb_dist_in1k', 'xcit_small_12_p16_224.fb_in1k', 'xcit_small_12_p16_384.fb_dist_in1k', 'xcit_small_24_p8_224.fb_dist_in1k', 'xcit_small_24_p8_224.fb_in1k', 'xcit_small_24_p8_384.fb_dist_in1k', 'xcit_small_24_p16_224.fb_dist_in1k', 'xcit_small_24_p16_224.fb_in1k', 'xcit_small_24_p16_384.fb_dist_in1k', 'xcit_tiny_12_p8_224.fb_dist_in1k', 'xcit_tiny_12_p8_224.fb_in1k', 'xcit_tiny_12_p8_384.fb_dist_in1k', 'xcit_tiny_12_p16_224.fb_dist_in1k', 'xcit_tiny_12_p16_224.fb_in1k', 'xcit_tiny_12_p16_384.fb_dist_in1k', 'xcit_tiny_24_p8_224.fb_dist_in1k', 'xcit_tiny_24_p8_224.fb_in1k', 'xcit_tiny_24_p8_384.fb_dist_in1k', 'xcit_tiny_24_p16_224.fb_dist_in1k', 'xcit_tiny_24_p16_224.fb_in1k', 'xcit_tiny_24_p16_384.fb_dist_in1k']\n"
     ]
    }
   ],
   "source": [
    "# 허깅페이스 사전학습 모델 종류 \n",
    "\n",
    "pretrained_model_list = timm.list_models(pretrained=True)\n",
    "\n",
    "print(pretrained_model_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {\n",
    "    \"train\": train_loader,\n",
    "    \"val\": val_loader\n",
    "}\n",
    "\n",
    "# 데이터셋 사이즈  \n",
    "# 폴더가 분리되어 있는 경우 len(train_ds)로도 가능\n",
    "# 폴더가 합쳐져있는 경우 get sampler에서 리턴받은 train_len,val_len으로 가능 \n",
    "dataset_sizes = {\n",
    "    \"train\": len(train_ds),\n",
    "    \"val\": len(test_ds)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timm import optim,scheduler\n",
    "\n",
    "# 최신 논문 구현에서는 Train one epoch, Validate one epoch 함수를 반복하는 형태로 진행\n",
    "# 제가 한 구현은 Train/validate 모두 하나의 함수에서 진행\n",
    "\n",
    "# GPU 사용 여부\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train_model(model, criterion, num_epochs=config['epochs']):\n",
    "    \n",
    "    since = time.time() # 시작 시간\n",
    "    best_model_wts = copy.deepcopy(model.state_dict()) #\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    '''\n",
    "    Transformer 학습은 AdamW, CosineLRS를 사용하는 것이 좋다고 알려져 있음\n",
    "    '''\n",
    "\n",
    "    #optimizer = torch.optim.Adam(model.parameters(), lr=1e-4) # torch에서 제공하는 Opimizer \n",
    "    optimizer = timm.optim.AdamW(model.parameters(), lr=1e-3,weight_decay=2e-3) # Hugging Face에서 제공하는 Optimizer\n",
    "    scheduler = timm.scheduler.cosine_lr.CosineLRScheduler(optimizer,t_initial=num_epochs,warmup_t=20)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print(\"-\"*10)\n",
    "        \n",
    "        for phase in ['train', 'val']: # We do training and validation phase per epoch\n",
    "            if phase == 'train':\n",
    "                model.train() # model to training mode\n",
    "            else:\n",
    "                model.eval() # model to evaluate \n",
    "            \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0.0\n",
    "\n",
    "            \n",
    "            '''\n",
    "            Mixed Precision 연산은 FP(floating point)16과 FP32를 혼합하여 연산하는 것\n",
    "            예전에는 amp라는 모듈을 따로 설치해야 했지만, 최근에는 torch에서 제공해줌\n",
    "\n",
    "            FP16은 FP32에 비해 메모리 사용량이 절반으로 줄어들고, 연산 속도가 빨라짐\n",
    "            특정 연산은 FP32로 해야하는 경우가 있음 -> Loss/Gradient 계산\n",
    "\n",
    "            참고자료 : \n",
    "            https://velog.io/@twinjuy/Auto-Mixed-Precision%EC%9D%B4%EB%9E%80\n",
    "            https://computing-jhson.tistory.com/37\n",
    "\n",
    "            주의할점:\n",
    "            모델 연산 중 나누기가 있는 경우(ex. 확률화)\n",
    "            Loss 값이 Nan으로 나오는 경우가 있음, 이러한 경우는 0으로 나눠서 발생하는 문제\n",
    "\n",
    "            '''\n",
    "            \n",
    "            # mixed precision 연산을 위한 scaler\n",
    "            # scaler는 FP16 연산으로 gradient가 0이 되는 것을 방지하기 위해 scale factor를 곱해주는 역할\n",
    "            scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "            # 한 Iteration마다 데이터를 불러와서 학습\n",
    "            for inputs,labels in tqdm(dataloaders[phase]):\n",
    "                if phase == 'train':\n",
    "                    # Huggingface에서 제공하는 Mixup 모듈\n",
    "                    # 하이퍼파라미터는 Transformer 논문들에서 고정적으로 쓰는 값ㅇ르 사용\n",
    "                    # Mixup은 모듈은 정수 레이블을 사용하더라도 원핫으로 모두 변환해줌\n",
    "                    mx_fn = Mixup(mixup_alpha=0.8, cutmix_alpha=1.0, num_classes=config['num_classes'])\n",
    "                    inputs, labels = mx_fn(inputs, labels)\n",
    "                    \n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "                    \n",
    "                    # Validation에서는 Mixup을 사용하지 않음\n",
    "                else:\n",
    "                    # ImageFolder에서 불러온 레이블은 정수레이블이므로 Mixup결과 레이블에 맞춰 one-hot encoding으로 변환\n",
    "                    labels = F.one_hot(labels, num_classes=config['num_classes'])\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "\n",
    "               \n",
    "                # 한 iteration 학습 전 gradient 초기화\n",
    "                # Pytorch에서는 gradients값들을 추후에 backward를 해줄때 계속 더해주기 때문\n",
    "                optimizer.zero_grad() \n",
    "                \n",
    "                with torch.autocast(device_type = 'cuda',dtype=torch.float16):\n",
    "                    outputs,_,_ = model(inputs) \n",
    "                    _, preds = torch.max(outputs, 1) # max value, index 리턴-> 값은 필요없음\n",
    "                    \n",
    "                    loss = criterion(outputs, labels) # loss 계산\n",
    "\n",
    "                if phase == 'train':\n",
    "                    # Loss 값 scaling\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "\n",
    "                # 한 에폭 학습 후 loss, accuracy 계산\n",
    "\n",
    "                # loss.item() -> Tensor 변수에서 값만 가져오기\n",
    "                # inputs.size(0) -> batch size\n",
    "                # loss*batch_size -> batch size만큼의 loss를 더해줌, 여기에 이후 데이터셋 크기만큼 나누면 iteration당 평균 loss 계산 가능 \n",
    "                running_loss += loss.item() * inputs.size(0) \n",
    "                running_corrects += torch.sum(preds == torch.argmax(labels,dim=-1)) \n",
    "\n",
    "            if phase == 'train':\n",
    "                scheduler.step(epoch) # step at end of epoch\n",
    "            \n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            \n",
    "            epoch_acc =  running_corrects / dataset_sizes[phase]\n",
    "            \n",
    "            \n",
    "            print(\"{} Loss: {:.4f} Acc: {:.4f}\".format(phase, epoch_loss, epoch_acc))\n",
    "            \n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict()) # keep the best validation accuracy model\n",
    "\n",
    "                model.load_state_dict(best_model_wts)\n",
    "                torch.save(model,'test_model')                    \n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since # slight error\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print(\"Best Val Acc: {:.4f}\".format(best_acc))\n",
    "    \n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/299\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1573 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Caught OSError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"c:\\Users\\user\\anaconda3\\envs\\Capsule\\lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"c:\\Users\\user\\anaconda3\\envs\\Capsule\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 58, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"c:\\Users\\user\\anaconda3\\envs\\Capsule\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 58, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"c:\\Users\\user\\anaconda3\\envs\\Capsule\\lib\\site-packages\\torchvision\\datasets\\folder.py\", line 229, in __getitem__\n    sample = self.loader(path)\n  File \"c:\\Users\\user\\anaconda3\\envs\\Capsule\\lib\\site-packages\\torchvision\\datasets\\folder.py\", line 268, in default_loader\n    return pil_loader(path)\n  File \"c:\\Users\\user\\anaconda3\\envs\\Capsule\\lib\\site-packages\\torchvision\\datasets\\folder.py\", line 248, in pil_loader\n    return img.convert(\"RGB\")\n  File \"c:\\Users\\user\\anaconda3\\envs\\Capsule\\lib\\site-packages\\PIL\\Image.py\", line 937, in convert\n    self.load()\n  File \"c:\\Users\\user\\anaconda3\\envs\\Capsule\\lib\\site-packages\\PIL\\ImageFile.py\", line 266, in load\n    raise OSError(msg)\nOSError: image file is truncated (13 bytes not processed)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32md:\\Coding\\DeepLearning_torch\\Pretrain.ipynb 셀 17\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Coding/DeepLearning_torch/Pretrain.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m criterion \u001b[39m=\u001b[39m SoftTargetCrossEntropy() \u001b[39m# 아니면 mixup에서 label smoothing을 적용해도 됨\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Coding/DeepLearning_torch/Pretrain.ipynb#X21sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m criterion \u001b[39m=\u001b[39m criterion\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Coding/DeepLearning_torch/Pretrain.ipynb#X21sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m model_ft \u001b[39m=\u001b[39m train_model(model, criterion) \n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Coding/DeepLearning_torch/Pretrain.ipynb#X21sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m model_ft\u001b[39m.\u001b[39meval()\n",
      "\u001b[1;32md:\\Coding\\DeepLearning_torch\\Pretrain.ipynb 셀 17\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Coding/DeepLearning_torch/Pretrain.ipynb#X21sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m scaler \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mamp\u001b[39m.\u001b[39mGradScaler()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Coding/DeepLearning_torch/Pretrain.ipynb#X21sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m \u001b[39m# 한 Iteration마다 데이터를 불러와서 학습\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Coding/DeepLearning_torch/Pretrain.ipynb#X21sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39mfor\u001b[39;00m inputs,labels \u001b[39min\u001b[39;00m tqdm(dataloaders[phase]):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Coding/DeepLearning_torch/Pretrain.ipynb#X21sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m     \u001b[39mif\u001b[39;00m phase \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Coding/DeepLearning_torch/Pretrain.ipynb#X21sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m         \u001b[39m# Huggingface에서 제공하는 Mixup 모듈\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Coding/DeepLearning_torch/Pretrain.ipynb#X21sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m         \u001b[39m# 하이퍼파라미터는 Transformer 논문들에서 고정적으로 쓰는 값ㅇ르 사용\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Coding/DeepLearning_torch/Pretrain.ipynb#X21sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m         \u001b[39m# Mixup은 모듈은 정수 레이블을 사용하더라도 원핫으로 모두 변환해줌\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Coding/DeepLearning_torch/Pretrain.ipynb#X21sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m         mx_fn \u001b[39m=\u001b[39m Mixup(mixup_alpha\u001b[39m=\u001b[39m\u001b[39m0.8\u001b[39m, cutmix_alpha\u001b[39m=\u001b[39m\u001b[39m1.0\u001b[39m, num_classes\u001b[39m=\u001b[39mconfig[\u001b[39m'\u001b[39m\u001b[39mnum_classes\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\Capsule\\lib\\site-packages\\tqdm\\std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1175\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[0;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1178\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[0;32m   1179\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m   1180\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1181\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\Capsule\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\Capsule\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1333\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1331\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1332\u001b[0m     \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task_info[idx]\n\u001b[1;32m-> 1333\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_data(data)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\Capsule\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1359\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1357\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_put_index()\n\u001b[0;32m   1358\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[1;32m-> 1359\u001b[0m     data\u001b[39m.\u001b[39;49mreraise()\n\u001b[0;32m   1360\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\Capsule\\lib\\site-packages\\torch\\_utils.py:543\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    539\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m    540\u001b[0m     \u001b[39m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[0;32m    541\u001b[0m     \u001b[39m# instantiate since we don't know how to\u001b[39;00m\n\u001b[0;32m    542\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 543\u001b[0m \u001b[39mraise\u001b[39;00m exception\n",
      "\u001b[1;31mOSError\u001b[0m: Caught OSError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"c:\\Users\\user\\anaconda3\\envs\\Capsule\\lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"c:\\Users\\user\\anaconda3\\envs\\Capsule\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 58, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"c:\\Users\\user\\anaconda3\\envs\\Capsule\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 58, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"c:\\Users\\user\\anaconda3\\envs\\Capsule\\lib\\site-packages\\torchvision\\datasets\\folder.py\", line 229, in __getitem__\n    sample = self.loader(path)\n  File \"c:\\Users\\user\\anaconda3\\envs\\Capsule\\lib\\site-packages\\torchvision\\datasets\\folder.py\", line 268, in default_loader\n    return pil_loader(path)\n  File \"c:\\Users\\user\\anaconda3\\envs\\Capsule\\lib\\site-packages\\torchvision\\datasets\\folder.py\", line 248, in pil_loader\n    return img.convert(\"RGB\")\n  File \"c:\\Users\\user\\anaconda3\\envs\\Capsule\\lib\\site-packages\\PIL\\Image.py\", line 937, in convert\n    self.load()\n  File \"c:\\Users\\user\\anaconda3\\envs\\Capsule\\lib\\site-packages\\PIL\\ImageFile.py\", line 266, in load\n    raise OSError(msg)\nOSError: image file is truncated (13 bytes not processed)\n"
     ]
    }
   ],
   "source": [
    "criterion = SoftTargetCrossEntropy() # 아니면 mixup에서 label smoothing을 적용해도 됨\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "model_ft = train_model(model, criterion) \n",
    "\n",
    "model_ft.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_data = datasets.ImageFolder(root= 'kproduct_test', transform = T.Compose([ T.Resize((224,224)), T.ToTensor(),\n",
    "            T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))]))\n",
    "\n",
    "    test_loader = DataLoader(test_data, batch_size=32, sampler=None ,shuffle=False, num_workers=4) \n",
    "\n",
    "    model = torch.load('test_model').to(device)\n",
    "    torchsummary.summary(model, (3, 224, 224))\n",
    "    model.eval()\n",
    "\n",
    "    criterion = SoftTargetCrossEntropy()\n",
    "    criterion = criterion.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(test_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Loss 계산\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # ACC 계산\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total = labels.size(0)\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            accuracy = correct / total\n",
    "        \n",
    "                \n",
    "    print(\"Loss: {:.4f} Acc: {:.4f}\".format(loss, accuracy))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Capsule",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
